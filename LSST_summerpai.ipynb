{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSST_summerpai.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbLEJe-391ed",
        "colab_type": "code",
        "outputId": "7622a00f-1506-494f-ca81-034742111d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        }
      },
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install numpy \n",
        "!pip3 install matplotlib\n",
        "!pip3 install pandas"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.3.0.post4 from http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl (592.3MB)\n",
            "\u001b[K     |████████████████████████████████| 592.3MB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (1.16.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (3.13)\n",
            "\u001b[31mERROR: torchvision 0.3.0 has requirement torch>=1.1.0, but you'll have torch 0.3.0.post4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.54 has requirement torch>=1.0.0, but you'll have torch 0.3.0.post4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.1.0\n",
            "    Uninstalling torch-1.1.0:\n",
            "      Successfully uninstalled torch-1.1.0\n",
            "Successfully installed torch-0.3.0.post4\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Collecting torch>=1.1.0 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/60/f685fb2cfb3088736bafbc9bdbb455327bdc8906b606da9c9a81bae1c81e/torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9MB)\n",
            "\u001b[K     |████████████████████████████████| 676.9MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 0.3.0.post4\n",
            "    Uninstalling torch-0.3.0.post4:\n",
            "      Successfully uninstalled torch-0.3.0.post4\n",
            "Successfully installed torch-1.1.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.16.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8-uO2T1_N4m",
        "colab_type": "code",
        "outputId": "dea669f5-98fe-467f-9df0-454d90232946",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "gc = 1 # Set to 1 if using Google Colab, 0 if using Jupyter Notebook.\n",
        "if gc == 1:\n",
        "  from google.colab import files\n",
        "  files.upload()\n",
        "\n",
        "print(\"GPU Available: \", torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b3ee4f8f-4dc0-485f-af4d-46ca6245b970\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-b3ee4f8f-4dc0-485f-af4d-46ca6245b970\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving training_set.csv to training_set.csv\n",
            "Saving training_set_metadata.csv to training_set_metadata.csv\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWPrrtd5-zQT",
        "colab_type": "code",
        "outputId": "a2a4fc3b-27fe-4d54-aa1d-c8548f6a6270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Show available files\n",
        "! ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  training_set.csv  training_set_metadata.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn3xXfA8BJ4C",
        "colab_type": "code",
        "outputId": "b7986774-f6db-479b-f76d-986c669ca2eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# Load in data\n",
        "\n",
        "train_data = pd.read_csv('training_set.csv')\n",
        "train_metadata = pd.read_csv('training_set_metadata.csv')\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>object_id</th>\n",
              "      <th>mjd</th>\n",
              "      <th>passband</th>\n",
              "      <th>flux</th>\n",
              "      <th>flux_err</th>\n",
              "      <th>detected</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>615</td>\n",
              "      <td>59750.4229</td>\n",
              "      <td>2</td>\n",
              "      <td>-544.810303</td>\n",
              "      <td>3.622952</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>615</td>\n",
              "      <td>59750.4306</td>\n",
              "      <td>1</td>\n",
              "      <td>-816.434326</td>\n",
              "      <td>5.553370</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>615</td>\n",
              "      <td>59750.4383</td>\n",
              "      <td>3</td>\n",
              "      <td>-471.385529</td>\n",
              "      <td>3.801213</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>615</td>\n",
              "      <td>59750.4450</td>\n",
              "      <td>4</td>\n",
              "      <td>-388.984985</td>\n",
              "      <td>11.395031</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>615</td>\n",
              "      <td>59752.4070</td>\n",
              "      <td>2</td>\n",
              "      <td>-681.858887</td>\n",
              "      <td>4.041204</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   object_id         mjd  passband        flux   flux_err  detected\n",
              "0        615  59750.4229         2 -544.810303   3.622952         1\n",
              "1        615  59750.4306         1 -816.434326   5.553370         1\n",
              "2        615  59750.4383         3 -471.385529   3.801213         1\n",
              "3        615  59750.4450         4 -388.984985  11.395031         1\n",
              "4        615  59752.4070         2 -681.858887   4.041204         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jNbNhSYBz2Q",
        "colab_type": "code",
        "outputId": "26490c15-705e-4490-e04a-c181537b14b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "# Data Visualization for 2 sample data points\n",
        "\n",
        "sample_data_1 = train_data[train_data.object_id == 615][train_data.passband == 1]\n",
        "sample_data_2 = train_data[train_data.object_id == 745][train_data.passband == 1]\n",
        "plt.scatter(sample_data_1['mjd'], sample_data_1['flux'], label = '615')\n",
        "plt.scatter(sample_data_2['mjd'], sample_data_2['flux'], label = '745')\n",
        "\n",
        "plt.title('Flux vs Time (Light Curves)')\n",
        "plt.xlabel('Time (Mjd)')\n",
        "plt.ylabel('Flux')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe17cd7a828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEWCAYAAACqitpwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM7PM/X8rk+EQ6riaXwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNPWnvvuDcSO",
        "colab_type": "code",
        "outputId": "54962d3f-99c4-4493-e245-26c351555a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "# Feature Engineering/Data Processing\n",
        "train_data['flux_fract_unc'] = train_data['flux']/train_data['flux_err']\n",
        "train_data['flux_by_flux_ratio_sq'] = train_data['flux']*np.power(train_data['flux_fract_unc'], 2)\n",
        "\n",
        "# Aggregate Data\n",
        "aggs = {\n",
        "    'mjd' : ['min', 'max', 'size'],\n",
        "    'flux' : ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
        "    'flux_err' : ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
        "    'detected' : ['mean'],\n",
        "    'flux_fract_unc' : ['sum', 'skew'],\n",
        "    'flux_by_flux_ratio_sq' : ['sum', 'skew']\n",
        "}\n",
        "\n",
        "agg_data = train_data.groupby(['object_id']).agg(aggs)\n",
        "new_columns = [\n",
        "    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n",
        "              ]\n",
        "\n",
        "agg_data.columns = new_columns\n",
        "agg_data['mjd_diff'] = agg_data['mjd_max'] - agg_data['mjd_min']\n",
        "agg_data['flux_diff'] = agg_data['flux_max'] - agg_data['flux_min']\n",
        "agg_data['flux_diff_mean'] = agg_data['flux_diff']/agg_data['flux_mean']\n",
        "agg_data['flux_by_time'] = agg_data['flux_diff']/agg_data['mjd_diff']\n",
        "\n",
        "agg_data.head()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mjd_min</th>\n",
              "      <th>mjd_max</th>\n",
              "      <th>mjd_size</th>\n",
              "      <th>flux_min</th>\n",
              "      <th>flux_max</th>\n",
              "      <th>flux_mean</th>\n",
              "      <th>flux_median</th>\n",
              "      <th>flux_std</th>\n",
              "      <th>flux_skew</th>\n",
              "      <th>flux_err_min</th>\n",
              "      <th>flux_err_max</th>\n",
              "      <th>flux_err_mean</th>\n",
              "      <th>flux_err_median</th>\n",
              "      <th>flux_err_std</th>\n",
              "      <th>flux_err_skew</th>\n",
              "      <th>detected_mean</th>\n",
              "      <th>flux_fract_unc_sum</th>\n",
              "      <th>flux_fract_unc_skew</th>\n",
              "      <th>flux_by_flux_ratio_sq_sum</th>\n",
              "      <th>flux_by_flux_ratio_sq_skew</th>\n",
              "      <th>mjd_diff</th>\n",
              "      <th>flux_diff</th>\n",
              "      <th>flux_diff_mean</th>\n",
              "      <th>flux_by_time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>object_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>615</th>\n",
              "      <td>59750.4229</td>\n",
              "      <td>60624.2132</td>\n",
              "      <td>352</td>\n",
              "      <td>-1100.440063</td>\n",
              "      <td>660.626343</td>\n",
              "      <td>-123.096998</td>\n",
              "      <td>-89.477524</td>\n",
              "      <td>394.109851</td>\n",
              "      <td>-0.349540</td>\n",
              "      <td>2.130510</td>\n",
              "      <td>12.845472</td>\n",
              "      <td>4.482743</td>\n",
              "      <td>3.835269</td>\n",
              "      <td>1.744747</td>\n",
              "      <td>1.623740</td>\n",
              "      <td>0.946023</td>\n",
              "      <td>-9060.302923</td>\n",
              "      <td>0.074759</td>\n",
              "      <td>-9.601766e+08</td>\n",
              "      <td>-1.414322</td>\n",
              "      <td>873.7903</td>\n",
              "      <td>1761.066406</td>\n",
              "      <td>-14.306331</td>\n",
              "      <td>2.015434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>713</th>\n",
              "      <td>59825.2600</td>\n",
              "      <td>60674.0798</td>\n",
              "      <td>350</td>\n",
              "      <td>-14.735178</td>\n",
              "      <td>14.770886</td>\n",
              "      <td>-1.423351</td>\n",
              "      <td>-0.873033</td>\n",
              "      <td>6.471144</td>\n",
              "      <td>0.014989</td>\n",
              "      <td>0.639458</td>\n",
              "      <td>9.115748</td>\n",
              "      <td>2.359620</td>\n",
              "      <td>1.998217</td>\n",
              "      <td>1.509888</td>\n",
              "      <td>1.633246</td>\n",
              "      <td>0.171429</td>\n",
              "      <td>-294.371461</td>\n",
              "      <td>-0.527471</td>\n",
              "      <td>-2.875087e+04</td>\n",
              "      <td>-3.454554</td>\n",
              "      <td>848.8198</td>\n",
              "      <td>29.506064</td>\n",
              "      <td>-20.730002</td>\n",
              "      <td>0.034761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>730</th>\n",
              "      <td>59798.3205</td>\n",
              "      <td>60652.1660</td>\n",
              "      <td>330</td>\n",
              "      <td>-19.159811</td>\n",
              "      <td>47.310059</td>\n",
              "      <td>2.267434</td>\n",
              "      <td>0.409172</td>\n",
              "      <td>8.022239</td>\n",
              "      <td>3.177854</td>\n",
              "      <td>0.695106</td>\n",
              "      <td>11.281384</td>\n",
              "      <td>2.471061</td>\n",
              "      <td>1.990851</td>\n",
              "      <td>1.721134</td>\n",
              "      <td>1.823726</td>\n",
              "      <td>0.069697</td>\n",
              "      <td>335.303259</td>\n",
              "      <td>3.553707</td>\n",
              "      <td>1.046502e+05</td>\n",
              "      <td>5.989138</td>\n",
              "      <td>853.8455</td>\n",
              "      <td>66.469870</td>\n",
              "      <td>29.315018</td>\n",
              "      <td>0.077848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>745</th>\n",
              "      <td>59770.3662</td>\n",
              "      <td>60624.0722</td>\n",
              "      <td>351</td>\n",
              "      <td>-15.494463</td>\n",
              "      <td>220.795212</td>\n",
              "      <td>8.909206</td>\n",
              "      <td>1.035895</td>\n",
              "      <td>27.558208</td>\n",
              "      <td>4.979826</td>\n",
              "      <td>0.567170</td>\n",
              "      <td>55.892746</td>\n",
              "      <td>2.555576</td>\n",
              "      <td>1.819875</td>\n",
              "      <td>3.537324</td>\n",
              "      <td>10.741655</td>\n",
              "      <td>0.173789</td>\n",
              "      <td>1645.743729</td>\n",
              "      <td>5.941321</td>\n",
              "      <td>1.439125e+07</td>\n",
              "      <td>11.141069</td>\n",
              "      <td>853.7060</td>\n",
              "      <td>236.289675</td>\n",
              "      <td>26.521968</td>\n",
              "      <td>0.276781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1124</th>\n",
              "      <td>59750.4229</td>\n",
              "      <td>60624.2132</td>\n",
              "      <td>352</td>\n",
              "      <td>-16.543753</td>\n",
              "      <td>143.600189</td>\n",
              "      <td>7.145702</td>\n",
              "      <td>1.141288</td>\n",
              "      <td>20.051722</td>\n",
              "      <td>4.406298</td>\n",
              "      <td>0.695277</td>\n",
              "      <td>11.383690</td>\n",
              "      <td>2.753004</td>\n",
              "      <td>2.214854</td>\n",
              "      <td>1.933837</td>\n",
              "      <td>1.794938</td>\n",
              "      <td>0.173295</td>\n",
              "      <td>1189.568855</td>\n",
              "      <td>4.736771</td>\n",
              "      <td>3.015599e+06</td>\n",
              "      <td>7.908174</td>\n",
              "      <td>873.7903</td>\n",
              "      <td>160.143942</td>\n",
              "      <td>22.411225</td>\n",
              "      <td>0.183275</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              mjd_min     mjd_max  ...  flux_diff_mean  flux_by_time\n",
              "object_id                          ...                              \n",
              "615        59750.4229  60624.2132  ...      -14.306331      2.015434\n",
              "713        59825.2600  60674.0798  ...      -20.730002      0.034761\n",
              "730        59798.3205  60652.1660  ...       29.315018      0.077848\n",
              "745        59770.3662  60624.0722  ...       26.521968      0.276781\n",
              "1124       59750.4229  60624.2132  ...       22.411225      0.183275\n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjKkuAAoGmeK",
        "colab_type": "code",
        "outputId": "a8fffd59-dec5-4395-efa5-37f10cc29e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "# Reset Index\n",
        "\n",
        "agg_data = agg_data.reset_index()\n",
        "agg_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>object_id</th>\n",
              "      <th>mjd_min</th>\n",
              "      <th>mjd_max</th>\n",
              "      <th>mjd_size</th>\n",
              "      <th>flux_min</th>\n",
              "      <th>flux_max</th>\n",
              "      <th>flux_mean</th>\n",
              "      <th>flux_median</th>\n",
              "      <th>flux_std</th>\n",
              "      <th>flux_skew</th>\n",
              "      <th>flux_err_min</th>\n",
              "      <th>flux_err_max</th>\n",
              "      <th>flux_err_mean</th>\n",
              "      <th>flux_err_median</th>\n",
              "      <th>flux_err_std</th>\n",
              "      <th>flux_err_skew</th>\n",
              "      <th>detected_mean</th>\n",
              "      <th>flux_fract_unc_sum</th>\n",
              "      <th>flux_fract_unc_skew</th>\n",
              "      <th>flux_by_flux_ratio_sq_sum</th>\n",
              "      <th>flux_by_flux_ratio_sq_skew</th>\n",
              "      <th>mjd_diff</th>\n",
              "      <th>flux_diff</th>\n",
              "      <th>flux_diff_mean</th>\n",
              "      <th>flux_by_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>615</td>\n",
              "      <td>59750.4229</td>\n",
              "      <td>60624.2132</td>\n",
              "      <td>352</td>\n",
              "      <td>-1100.440063</td>\n",
              "      <td>660.626343</td>\n",
              "      <td>-123.096998</td>\n",
              "      <td>-89.477524</td>\n",
              "      <td>394.109851</td>\n",
              "      <td>-0.349540</td>\n",
              "      <td>2.130510</td>\n",
              "      <td>12.845472</td>\n",
              "      <td>4.482743</td>\n",
              "      <td>3.835269</td>\n",
              "      <td>1.744747</td>\n",
              "      <td>1.623740</td>\n",
              "      <td>0.946023</td>\n",
              "      <td>-9060.302923</td>\n",
              "      <td>0.074759</td>\n",
              "      <td>-9.601766e+08</td>\n",
              "      <td>-1.414322</td>\n",
              "      <td>873.7903</td>\n",
              "      <td>1761.066406</td>\n",
              "      <td>-14.306331</td>\n",
              "      <td>2.015434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>713</td>\n",
              "      <td>59825.2600</td>\n",
              "      <td>60674.0798</td>\n",
              "      <td>350</td>\n",
              "      <td>-14.735178</td>\n",
              "      <td>14.770886</td>\n",
              "      <td>-1.423351</td>\n",
              "      <td>-0.873033</td>\n",
              "      <td>6.471144</td>\n",
              "      <td>0.014989</td>\n",
              "      <td>0.639458</td>\n",
              "      <td>9.115748</td>\n",
              "      <td>2.359620</td>\n",
              "      <td>1.998217</td>\n",
              "      <td>1.509888</td>\n",
              "      <td>1.633246</td>\n",
              "      <td>0.171429</td>\n",
              "      <td>-294.371461</td>\n",
              "      <td>-0.527471</td>\n",
              "      <td>-2.875087e+04</td>\n",
              "      <td>-3.454554</td>\n",
              "      <td>848.8198</td>\n",
              "      <td>29.506064</td>\n",
              "      <td>-20.730002</td>\n",
              "      <td>0.034761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>730</td>\n",
              "      <td>59798.3205</td>\n",
              "      <td>60652.1660</td>\n",
              "      <td>330</td>\n",
              "      <td>-19.159811</td>\n",
              "      <td>47.310059</td>\n",
              "      <td>2.267434</td>\n",
              "      <td>0.409172</td>\n",
              "      <td>8.022239</td>\n",
              "      <td>3.177854</td>\n",
              "      <td>0.695106</td>\n",
              "      <td>11.281384</td>\n",
              "      <td>2.471061</td>\n",
              "      <td>1.990851</td>\n",
              "      <td>1.721134</td>\n",
              "      <td>1.823726</td>\n",
              "      <td>0.069697</td>\n",
              "      <td>335.303259</td>\n",
              "      <td>3.553707</td>\n",
              "      <td>1.046502e+05</td>\n",
              "      <td>5.989138</td>\n",
              "      <td>853.8455</td>\n",
              "      <td>66.469870</td>\n",
              "      <td>29.315018</td>\n",
              "      <td>0.077848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>745</td>\n",
              "      <td>59770.3662</td>\n",
              "      <td>60624.0722</td>\n",
              "      <td>351</td>\n",
              "      <td>-15.494463</td>\n",
              "      <td>220.795212</td>\n",
              "      <td>8.909206</td>\n",
              "      <td>1.035895</td>\n",
              "      <td>27.558208</td>\n",
              "      <td>4.979826</td>\n",
              "      <td>0.567170</td>\n",
              "      <td>55.892746</td>\n",
              "      <td>2.555576</td>\n",
              "      <td>1.819875</td>\n",
              "      <td>3.537324</td>\n",
              "      <td>10.741655</td>\n",
              "      <td>0.173789</td>\n",
              "      <td>1645.743729</td>\n",
              "      <td>5.941321</td>\n",
              "      <td>1.439125e+07</td>\n",
              "      <td>11.141069</td>\n",
              "      <td>853.7060</td>\n",
              "      <td>236.289675</td>\n",
              "      <td>26.521968</td>\n",
              "      <td>0.276781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1124</td>\n",
              "      <td>59750.4229</td>\n",
              "      <td>60624.2132</td>\n",
              "      <td>352</td>\n",
              "      <td>-16.543753</td>\n",
              "      <td>143.600189</td>\n",
              "      <td>7.145702</td>\n",
              "      <td>1.141288</td>\n",
              "      <td>20.051722</td>\n",
              "      <td>4.406298</td>\n",
              "      <td>0.695277</td>\n",
              "      <td>11.383690</td>\n",
              "      <td>2.753004</td>\n",
              "      <td>2.214854</td>\n",
              "      <td>1.933837</td>\n",
              "      <td>1.794938</td>\n",
              "      <td>0.173295</td>\n",
              "      <td>1189.568855</td>\n",
              "      <td>4.736771</td>\n",
              "      <td>3.015599e+06</td>\n",
              "      <td>7.908174</td>\n",
              "      <td>873.7903</td>\n",
              "      <td>160.143942</td>\n",
              "      <td>22.411225</td>\n",
              "      <td>0.183275</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   object_id     mjd_min     mjd_max  ...    flux_diff  flux_diff_mean  flux_by_time\n",
              "0        615  59750.4229  60624.2132  ...  1761.066406      -14.306331      2.015434\n",
              "1        713  59825.2600  60674.0798  ...    29.506064      -20.730002      0.034761\n",
              "2        730  59798.3205  60652.1660  ...    66.469870       29.315018      0.077848\n",
              "3        745  59770.3662  60624.0722  ...   236.289675       26.521968      0.276781\n",
              "4       1124  59750.4229  60624.2132  ...   160.143942       22.411225      0.183275\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbuESJtgGuQ4",
        "colab_type": "code",
        "outputId": "ee3ddb5d-2a4d-4993-e4de-ef321aee130c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "# Merge aggregated data with metadata\n",
        "\n",
        "all_data = agg_data.merge(\n",
        "                        right=train_metadata,\n",
        "                        how='outer',\n",
        "                        on='object_id')\n",
        "all_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>object_id</th>\n",
              "      <th>mjd_min</th>\n",
              "      <th>mjd_max</th>\n",
              "      <th>mjd_size</th>\n",
              "      <th>flux_min</th>\n",
              "      <th>flux_max</th>\n",
              "      <th>flux_mean</th>\n",
              "      <th>flux_median</th>\n",
              "      <th>flux_std</th>\n",
              "      <th>flux_skew</th>\n",
              "      <th>flux_err_min</th>\n",
              "      <th>flux_err_max</th>\n",
              "      <th>flux_err_mean</th>\n",
              "      <th>flux_err_median</th>\n",
              "      <th>flux_err_std</th>\n",
              "      <th>flux_err_skew</th>\n",
              "      <th>detected_mean</th>\n",
              "      <th>flux_fract_unc_sum</th>\n",
              "      <th>flux_fract_unc_skew</th>\n",
              "      <th>flux_by_flux_ratio_sq_sum</th>\n",
              "      <th>flux_by_flux_ratio_sq_skew</th>\n",
              "      <th>mjd_diff</th>\n",
              "      <th>flux_diff</th>\n",
              "      <th>flux_diff_mean</th>\n",
              "      <th>flux_by_time</th>\n",
              "      <th>ra</th>\n",
              "      <th>decl</th>\n",
              "      <th>gal_l</th>\n",
              "      <th>gal_b</th>\n",
              "      <th>ddf</th>\n",
              "      <th>hostgal_specz</th>\n",
              "      <th>hostgal_photoz</th>\n",
              "      <th>hostgal_photoz_err</th>\n",
              "      <th>distmod</th>\n",
              "      <th>mwebv</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>615</td>\n",
              "      <td>59750.4229</td>\n",
              "      <td>60624.2132</td>\n",
              "      <td>352</td>\n",
              "      <td>-1100.440063</td>\n",
              "      <td>660.626343</td>\n",
              "      <td>-123.096998</td>\n",
              "      <td>-89.477524</td>\n",
              "      <td>394.109851</td>\n",
              "      <td>-0.349540</td>\n",
              "      <td>2.130510</td>\n",
              "      <td>12.845472</td>\n",
              "      <td>4.482743</td>\n",
              "      <td>3.835269</td>\n",
              "      <td>1.744747</td>\n",
              "      <td>1.623740</td>\n",
              "      <td>0.946023</td>\n",
              "      <td>-9060.302923</td>\n",
              "      <td>0.074759</td>\n",
              "      <td>-9.601766e+08</td>\n",
              "      <td>-1.414322</td>\n",
              "      <td>873.7903</td>\n",
              "      <td>1761.066406</td>\n",
              "      <td>-14.306331</td>\n",
              "      <td>2.015434</td>\n",
              "      <td>349.046051</td>\n",
              "      <td>-61.943836</td>\n",
              "      <td>320.796530</td>\n",
              "      <td>-51.753706</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.017</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>713</td>\n",
              "      <td>59825.2600</td>\n",
              "      <td>60674.0798</td>\n",
              "      <td>350</td>\n",
              "      <td>-14.735178</td>\n",
              "      <td>14.770886</td>\n",
              "      <td>-1.423351</td>\n",
              "      <td>-0.873033</td>\n",
              "      <td>6.471144</td>\n",
              "      <td>0.014989</td>\n",
              "      <td>0.639458</td>\n",
              "      <td>9.115748</td>\n",
              "      <td>2.359620</td>\n",
              "      <td>1.998217</td>\n",
              "      <td>1.509888</td>\n",
              "      <td>1.633246</td>\n",
              "      <td>0.171429</td>\n",
              "      <td>-294.371461</td>\n",
              "      <td>-0.527471</td>\n",
              "      <td>-2.875087e+04</td>\n",
              "      <td>-3.454554</td>\n",
              "      <td>848.8198</td>\n",
              "      <td>29.506064</td>\n",
              "      <td>-20.730002</td>\n",
              "      <td>0.034761</td>\n",
              "      <td>53.085938</td>\n",
              "      <td>-27.784405</td>\n",
              "      <td>223.525509</td>\n",
              "      <td>-54.460748</td>\n",
              "      <td>1</td>\n",
              "      <td>1.8181</td>\n",
              "      <td>1.6267</td>\n",
              "      <td>0.2552</td>\n",
              "      <td>45.4063</td>\n",
              "      <td>0.007</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>730</td>\n",
              "      <td>59798.3205</td>\n",
              "      <td>60652.1660</td>\n",
              "      <td>330</td>\n",
              "      <td>-19.159811</td>\n",
              "      <td>47.310059</td>\n",
              "      <td>2.267434</td>\n",
              "      <td>0.409172</td>\n",
              "      <td>8.022239</td>\n",
              "      <td>3.177854</td>\n",
              "      <td>0.695106</td>\n",
              "      <td>11.281384</td>\n",
              "      <td>2.471061</td>\n",
              "      <td>1.990851</td>\n",
              "      <td>1.721134</td>\n",
              "      <td>1.823726</td>\n",
              "      <td>0.069697</td>\n",
              "      <td>335.303259</td>\n",
              "      <td>3.553707</td>\n",
              "      <td>1.046502e+05</td>\n",
              "      <td>5.989138</td>\n",
              "      <td>853.8455</td>\n",
              "      <td>66.469870</td>\n",
              "      <td>29.315018</td>\n",
              "      <td>0.077848</td>\n",
              "      <td>33.574219</td>\n",
              "      <td>-6.579593</td>\n",
              "      <td>170.455585</td>\n",
              "      <td>-61.548219</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2320</td>\n",
              "      <td>0.2262</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>40.2561</td>\n",
              "      <td>0.021</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>745</td>\n",
              "      <td>59770.3662</td>\n",
              "      <td>60624.0722</td>\n",
              "      <td>351</td>\n",
              "      <td>-15.494463</td>\n",
              "      <td>220.795212</td>\n",
              "      <td>8.909206</td>\n",
              "      <td>1.035895</td>\n",
              "      <td>27.558208</td>\n",
              "      <td>4.979826</td>\n",
              "      <td>0.567170</td>\n",
              "      <td>55.892746</td>\n",
              "      <td>2.555576</td>\n",
              "      <td>1.819875</td>\n",
              "      <td>3.537324</td>\n",
              "      <td>10.741655</td>\n",
              "      <td>0.173789</td>\n",
              "      <td>1645.743729</td>\n",
              "      <td>5.941321</td>\n",
              "      <td>1.439125e+07</td>\n",
              "      <td>11.141069</td>\n",
              "      <td>853.7060</td>\n",
              "      <td>236.289675</td>\n",
              "      <td>26.521968</td>\n",
              "      <td>0.276781</td>\n",
              "      <td>0.189873</td>\n",
              "      <td>-45.586655</td>\n",
              "      <td>328.254458</td>\n",
              "      <td>-68.969298</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3037</td>\n",
              "      <td>0.2813</td>\n",
              "      <td>1.1523</td>\n",
              "      <td>40.7951</td>\n",
              "      <td>0.007</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1124</td>\n",
              "      <td>59750.4229</td>\n",
              "      <td>60624.2132</td>\n",
              "      <td>352</td>\n",
              "      <td>-16.543753</td>\n",
              "      <td>143.600189</td>\n",
              "      <td>7.145702</td>\n",
              "      <td>1.141288</td>\n",
              "      <td>20.051722</td>\n",
              "      <td>4.406298</td>\n",
              "      <td>0.695277</td>\n",
              "      <td>11.383690</td>\n",
              "      <td>2.753004</td>\n",
              "      <td>2.214854</td>\n",
              "      <td>1.933837</td>\n",
              "      <td>1.794938</td>\n",
              "      <td>0.173295</td>\n",
              "      <td>1189.568855</td>\n",
              "      <td>4.736771</td>\n",
              "      <td>3.015599e+06</td>\n",
              "      <td>7.908174</td>\n",
              "      <td>873.7903</td>\n",
              "      <td>160.143942</td>\n",
              "      <td>22.411225</td>\n",
              "      <td>0.183275</td>\n",
              "      <td>352.711273</td>\n",
              "      <td>-63.823658</td>\n",
              "      <td>316.922299</td>\n",
              "      <td>-51.059403</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1934</td>\n",
              "      <td>0.2415</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>40.4166</td>\n",
              "      <td>0.024</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   object_id     mjd_min     mjd_max  ...  distmod  mwebv  target\n",
              "0        615  59750.4229  60624.2132  ...      NaN  0.017      92\n",
              "1        713  59825.2600  60674.0798  ...  45.4063  0.007      88\n",
              "2        730  59798.3205  60652.1660  ...  40.2561  0.021      42\n",
              "3        745  59770.3662  60624.0722  ...  40.7951  0.007      90\n",
              "4       1124  59750.4229  60624.2132  ...  40.4166  0.024      90\n",
              "\n",
              "[5 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGCl-tU_HXZp",
        "colab_type": "code",
        "outputId": "36e7dacb-fc93-4161-9448-590c79b15bc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Convert targets to be compatible with model.\n",
        "\n",
        "y = all_data['target']\n",
        "classes = sorted(y.unique())\n",
        "\n",
        "print('Unique Classes: ', classes)\n",
        "\n",
        "class_map = dict()\n",
        "for i, val in enumerate(classes):\n",
        "   class_map[val] = i\n",
        "    \n",
        "# [6, 15, 16, 42] -> [0, 1, 2, 3] \n",
        "def to_categorical(y, num_classes):\n",
        "  \"\"\"1-hot encode\"\"\"\n",
        "  return np.eye(num_classes, dtype='uint8')[y]\n",
        "\n",
        "y_map = np.array([class_map[val] for val in y])\n",
        "y_categorical = to_categorical(y_map, 14)\n",
        "\n",
        "print(\"Target Shape:\", y_categorical.shape)\n",
        "print(\"Categorized examples: \", y_categorical[0:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique Classes:  [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
            "(7848, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiO47y5eKORY",
        "colab_type": "code",
        "outputId": "5615826d-036b-4481-a55a-9070380ec11c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Final Touches\n",
        "\n",
        "#Remove Unnecessary Data\n",
        "del all_data['object_id'], all_data['distmod'], all_data['ra'], all_data['decl'], all_data['gal_l'], all_data['gal_b']\n",
        "\n",
        "#Remove Invalid/Empty Data\n",
        "train_mean = all_data.mean(axis=0) \n",
        "all_data.fillna(train_mean, inplace=True)\n",
        "\n",
        "#Format Data for Learning\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "ss = StandardScaler()\n",
        "myData = ss.fit_transform(all_data)\n",
        "\n",
        "print('Data shape: ', myData.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data shape:  (7848, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W1ExoUPqNu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self, size_inputs, size_outputs):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(size_inputs, 24), #30 input vars -> 24 unique features\n",
        "      nn.BatchNorm1d(24),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(24, 16), #24 features -> 16 features\n",
        "      nn.BatchNorm1d(16),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(16, size_outputs), #16 features -> 14 possible outputs\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    results = self.net(inputs)\n",
        "    return results\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DfkerysswUE",
        "colab_type": "code",
        "outputId": "32322012-6023-4dff-85c6-621531926ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "size_inputs = myData.shape[1]\n",
        "size_outputs = y_categorical.shape[1]\n",
        "\n",
        "# Initialize Model's Neural Network, Loss Function, and Optimizer\n",
        "myNN = NeuralNetwork(size_inputs, size_outputs)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(myNN.parameters(), lr=0.001)\n",
        "\n",
        "# Split data into testing and training sets.\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(myData, y_categorical, test_size=0.2)\n",
        "\n",
        "# Create Tensor Objects\n",
        "train_x = torch.FloatTensor(xTrain)\n",
        "train_y = torch.FloatTensor(yTrain)\n",
        "test_x = torch.FloatTensor(xTest)\n",
        "test_y = torch.FloatTensor(yTest)\n",
        "\n",
        "# Make sure model is working. \n",
        "results = myNN(train_x)\n",
        "error = criterion(results, train_y)\n",
        "print(\"Results: \", results[0:5])\n",
        "print(\"Error:\", error)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results:  tensor([[0.4319, 0.5652, 0.5003, 0.4641, 0.4341, 0.5177, 0.4419, 0.4616, 0.5082,\n",
            "         0.5476, 0.5723, 0.4900, 0.4712, 0.4863],\n",
            "        [0.4586, 0.5217, 0.5067, 0.4879, 0.4579, 0.4950, 0.4696, 0.4458, 0.4614,\n",
            "         0.5473, 0.6018, 0.5193, 0.4692, 0.5043],\n",
            "        [0.4357, 0.4997, 0.5565, 0.4173, 0.3953, 0.4851, 0.5516, 0.5147, 0.4687,\n",
            "         0.4344, 0.4681, 0.5920, 0.4042, 0.4108],\n",
            "        [0.5748, 0.4412, 0.3837, 0.4361, 0.6135, 0.3544, 0.2776, 0.5808, 0.6500,\n",
            "         0.5823, 0.5225, 0.7310, 0.5141, 0.4372],\n",
            "        [0.5276, 0.4927, 0.5182, 0.4105, 0.5831, 0.4233, 0.3922, 0.5564, 0.5692,\n",
            "         0.5029, 0.3942, 0.7266, 0.4060, 0.3687]], grad_fn=<SliceBackward>)\n",
            "Error: tensor(0.6833, grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwBnWdcsvaW3",
        "colab_type": "code",
        "outputId": "a8f8217c-c3d5-483b-db52-5d1b5be98b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Function to train or test model\n",
        "def train_test(network, inp, labels, optimizer=None, criterion=None, mode='train'):\n",
        "  preds = network(inp)\n",
        "  error = criterion(preds, labels)\n",
        "  \n",
        "  if mode == 'train':\n",
        "    optimizer.zero_grad()\n",
        "    error.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  return preds, error.item()\n",
        "\n",
        "# Training Length\n",
        "num_epochs = 500 \n",
        "\n",
        "# Store losses\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# Train and test at every epoch\n",
        "for i in range(num_epochs):\n",
        "  _, train_loss = train_test(myNN, train_x, train_y, optimizer, criterion, mode='train')\n",
        "  train_losses.append(train_loss)\n",
        "  \n",
        "  _, test_loss = train_test(myNN, test_x, test_y, criterion=criterion, mode='test')\n",
        "  test_losses.append(test_loss)\n",
        "  print(\"Epoch: \", i, \", Training Loss:\", train_loss, \"Test Loss:\", test_loss)\n",
        "\n",
        "  \n",
        "# Simple plot for losses\n",
        "plt.plot(train_losses, label='training loss')\n",
        "plt.plot(test_losses, label='test loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0 , Training Loss: 0.6687524914741516 Test Loss: 0.6639430522918701\n",
            "Epoch:  1 , Training Loss: 0.6662685871124268 Test Loss: 0.6618569493293762\n",
            "Epoch:  2 , Training Loss: 0.6638174057006836 Test Loss: 0.6597669720649719\n",
            "Epoch:  3 , Training Loss: 0.6614425778388977 Test Loss: 0.657658576965332\n",
            "Epoch:  4 , Training Loss: 0.6591002345085144 Test Loss: 0.6555445194244385\n",
            "Epoch:  5 , Training Loss: 0.6567552089691162 Test Loss: 0.6534290909767151\n",
            "Epoch:  6 , Training Loss: 0.6544646620750427 Test Loss: 0.6512951254844666\n",
            "Epoch:  7 , Training Loss: 0.6522075533866882 Test Loss: 0.6491531729698181\n",
            "Epoch:  8 , Training Loss: 0.6499757766723633 Test Loss: 0.6470009088516235\n",
            "Epoch:  9 , Training Loss: 0.6477611064910889 Test Loss: 0.644837498664856\n",
            "Epoch:  10 , Training Loss: 0.6455534100532532 Test Loss: 0.6426657438278198\n",
            "Epoch:  11 , Training Loss: 0.6433584094047546 Test Loss: 0.6404892802238464\n",
            "Epoch:  12 , Training Loss: 0.6411622762680054 Test Loss: 0.6383068561553955\n",
            "Epoch:  13 , Training Loss: 0.6389637589454651 Test Loss: 0.6361231207847595\n",
            "Epoch:  14 , Training Loss: 0.6367697715759277 Test Loss: 0.633942186832428\n",
            "Epoch:  15 , Training Loss: 0.6345829963684082 Test Loss: 0.6317555904388428\n",
            "Epoch:  16 , Training Loss: 0.632378339767456 Test Loss: 0.6295600533485413\n",
            "Epoch:  17 , Training Loss: 0.6301629543304443 Test Loss: 0.6273607611656189\n",
            "Epoch:  18 , Training Loss: 0.6279312372207642 Test Loss: 0.6251524090766907\n",
            "Epoch:  19 , Training Loss: 0.6256906390190125 Test Loss: 0.6229150891304016\n",
            "Epoch:  20 , Training Loss: 0.6234254240989685 Test Loss: 0.6206526756286621\n",
            "Epoch:  21 , Training Loss: 0.621151328086853 Test Loss: 0.6183764934539795\n",
            "Epoch:  22 , Training Loss: 0.6188560724258423 Test Loss: 0.616097092628479\n",
            "Epoch:  23 , Training Loss: 0.6165433526039124 Test Loss: 0.6138127446174622\n",
            "Epoch:  24 , Training Loss: 0.614224910736084 Test Loss: 0.6115166544914246\n",
            "Epoch:  25 , Training Loss: 0.6118912100791931 Test Loss: 0.6092056632041931\n",
            "Epoch:  26 , Training Loss: 0.6095348000526428 Test Loss: 0.6068850755691528\n",
            "Epoch:  27 , Training Loss: 0.6071757674217224 Test Loss: 0.6045384407043457\n",
            "Epoch:  28 , Training Loss: 0.6047948598861694 Test Loss: 0.6021817326545715\n",
            "Epoch:  29 , Training Loss: 0.602401614189148 Test Loss: 0.5998085141181946\n",
            "Epoch:  30 , Training Loss: 0.5999848246574402 Test Loss: 0.5974153280258179\n",
            "Epoch:  31 , Training Loss: 0.5975711941719055 Test Loss: 0.5950020551681519\n",
            "Epoch:  32 , Training Loss: 0.5951327085494995 Test Loss: 0.5925666093826294\n",
            "Epoch:  33 , Training Loss: 0.5926821827888489 Test Loss: 0.5901126861572266\n",
            "Epoch:  34 , Training Loss: 0.5902118682861328 Test Loss: 0.5876468420028687\n",
            "Epoch:  35 , Training Loss: 0.587731122970581 Test Loss: 0.5851683020591736\n",
            "Epoch:  36 , Training Loss: 0.5852451920509338 Test Loss: 0.5826825499534607\n",
            "Epoch:  37 , Training Loss: 0.5827398896217346 Test Loss: 0.5801891684532166\n",
            "Epoch:  38 , Training Loss: 0.5802230834960938 Test Loss: 0.5776805877685547\n",
            "Epoch:  39 , Training Loss: 0.5777034759521484 Test Loss: 0.5751653909683228\n",
            "Epoch:  40 , Training Loss: 0.5751784443855286 Test Loss: 0.572645902633667\n",
            "Epoch:  41 , Training Loss: 0.5726398825645447 Test Loss: 0.5701249837875366\n",
            "Epoch:  42 , Training Loss: 0.5700995922088623 Test Loss: 0.5676017999649048\n",
            "Epoch:  43 , Training Loss: 0.5675615668296814 Test Loss: 0.5650784373283386\n",
            "Epoch:  44 , Training Loss: 0.565004289150238 Test Loss: 0.5625554323196411\n",
            "Epoch:  45 , Training Loss: 0.5624427795410156 Test Loss: 0.5600379109382629\n",
            "Epoch:  46 , Training Loss: 0.5598881244659424 Test Loss: 0.5575249791145325\n",
            "Epoch:  47 , Training Loss: 0.557317852973938 Test Loss: 0.5550037026405334\n",
            "Epoch:  48 , Training Loss: 0.5547484755516052 Test Loss: 0.552487850189209\n",
            "Epoch:  49 , Training Loss: 0.55217444896698 Test Loss: 0.549963116645813\n",
            "Epoch:  50 , Training Loss: 0.5495922565460205 Test Loss: 0.5474449396133423\n",
            "Epoch:  51 , Training Loss: 0.5470104813575745 Test Loss: 0.5449247360229492\n",
            "Epoch:  52 , Training Loss: 0.5444187521934509 Test Loss: 0.5424025654792786\n",
            "Epoch:  53 , Training Loss: 0.5418296456336975 Test Loss: 0.539876401424408\n",
            "Epoch:  54 , Training Loss: 0.5392215847969055 Test Loss: 0.5373432636260986\n",
            "Epoch:  55 , Training Loss: 0.5366155505180359 Test Loss: 0.5348127484321594\n",
            "Epoch:  56 , Training Loss: 0.5340047478675842 Test Loss: 0.5322797298431396\n",
            "Epoch:  57 , Training Loss: 0.5313794016838074 Test Loss: 0.5297406315803528\n",
            "Epoch:  58 , Training Loss: 0.5287511348724365 Test Loss: 0.5271939039230347\n",
            "Epoch:  59 , Training Loss: 0.5261142253875732 Test Loss: 0.5246392488479614\n",
            "Epoch:  60 , Training Loss: 0.5234729647636414 Test Loss: 0.522076427936554\n",
            "Epoch:  61 , Training Loss: 0.5208163857460022 Test Loss: 0.519502580165863\n",
            "Epoch:  62 , Training Loss: 0.5181519389152527 Test Loss: 0.5169236063957214\n",
            "Epoch:  63 , Training Loss: 0.5154690146446228 Test Loss: 0.5143346786499023\n",
            "Epoch:  64 , Training Loss: 0.5127851963043213 Test Loss: 0.5117360949516296\n",
            "Epoch:  65 , Training Loss: 0.5100945234298706 Test Loss: 0.5091264843940735\n",
            "Epoch:  66 , Training Loss: 0.5073854327201843 Test Loss: 0.5065104365348816\n",
            "Epoch:  67 , Training Loss: 0.5046788454055786 Test Loss: 0.5038785934448242\n",
            "Epoch:  68 , Training Loss: 0.5019597411155701 Test Loss: 0.5012385845184326\n",
            "Epoch:  69 , Training Loss: 0.49923306703567505 Test Loss: 0.498588502407074\n",
            "Epoch:  70 , Training Loss: 0.49649423360824585 Test Loss: 0.4959286153316498\n",
            "Epoch:  71 , Training Loss: 0.49375876784324646 Test Loss: 0.4932717978954315\n",
            "Epoch:  72 , Training Loss: 0.49101027846336365 Test Loss: 0.49060386419296265\n",
            "Epoch:  73 , Training Loss: 0.48825693130493164 Test Loss: 0.4879375994205475\n",
            "Epoch:  74 , Training Loss: 0.48550209403038025 Test Loss: 0.48526713252067566\n",
            "Epoch:  75 , Training Loss: 0.48273617029190063 Test Loss: 0.4825822114944458\n",
            "Epoch:  76 , Training Loss: 0.47996577620506287 Test Loss: 0.4798879325389862\n",
            "Epoch:  77 , Training Loss: 0.4772014319896698 Test Loss: 0.4771880805492401\n",
            "Epoch:  78 , Training Loss: 0.4744310975074768 Test Loss: 0.4744868874549866\n",
            "Epoch:  79 , Training Loss: 0.4716482162475586 Test Loss: 0.471786230802536\n",
            "Epoch:  80 , Training Loss: 0.46887022256851196 Test Loss: 0.46908459067344666\n",
            "Epoch:  81 , Training Loss: 0.4660871624946594 Test Loss: 0.46638011932373047\n",
            "Epoch:  82 , Training Loss: 0.4633040428161621 Test Loss: 0.4636816382408142\n",
            "Epoch:  83 , Training Loss: 0.4605143070220947 Test Loss: 0.4609884023666382\n",
            "Epoch:  84 , Training Loss: 0.4577312469482422 Test Loss: 0.4583011865615845\n",
            "Epoch:  85 , Training Loss: 0.4549504518508911 Test Loss: 0.4556175768375397\n",
            "Epoch:  86 , Training Loss: 0.45217451453208923 Test Loss: 0.4529331922531128\n",
            "Epoch:  87 , Training Loss: 0.44940564036369324 Test Loss: 0.45025163888931274\n",
            "Epoch:  88 , Training Loss: 0.44664278626441956 Test Loss: 0.44757312536239624\n",
            "Epoch:  89 , Training Loss: 0.443887323141098 Test Loss: 0.44489189982414246\n",
            "Epoch:  90 , Training Loss: 0.4411381185054779 Test Loss: 0.44220468401908875\n",
            "Epoch:  91 , Training Loss: 0.438395231962204 Test Loss: 0.439522922039032\n",
            "Epoch:  92 , Training Loss: 0.4356544613838196 Test Loss: 0.436842143535614\n",
            "Epoch:  93 , Training Loss: 0.43292859196662903 Test Loss: 0.4341617822647095\n",
            "Epoch:  94 , Training Loss: 0.43021002411842346 Test Loss: 0.4314826726913452\n",
            "Epoch:  95 , Training Loss: 0.4275049865245819 Test Loss: 0.42880162596702576\n",
            "Epoch:  96 , Training Loss: 0.42480623722076416 Test Loss: 0.4261263310909271\n",
            "Epoch:  97 , Training Loss: 0.4221176505088806 Test Loss: 0.42345568537712097\n",
            "Epoch:  98 , Training Loss: 0.4194391369819641 Test Loss: 0.42079487442970276\n",
            "Epoch:  99 , Training Loss: 0.4167677164077759 Test Loss: 0.41813865303993225\n",
            "Epoch:  100 , Training Loss: 0.41410166025161743 Test Loss: 0.4154929220676422\n",
            "Epoch:  101 , Training Loss: 0.41144728660583496 Test Loss: 0.4128495752811432\n",
            "Epoch:  102 , Training Loss: 0.40880101919174194 Test Loss: 0.41021573543548584\n",
            "Epoch:  103 , Training Loss: 0.40616652369499207 Test Loss: 0.4075874090194702\n",
            "Epoch:  104 , Training Loss: 0.4035375416278839 Test Loss: 0.404975950717926\n",
            "Epoch:  105 , Training Loss: 0.40091684460639954 Test Loss: 0.4023738205432892\n",
            "Epoch:  106 , Training Loss: 0.3983073830604553 Test Loss: 0.39978790283203125\n",
            "Epoch:  107 , Training Loss: 0.3957146406173706 Test Loss: 0.3972146511077881\n",
            "Epoch:  108 , Training Loss: 0.39313241839408875 Test Loss: 0.39465782046318054\n",
            "Epoch:  109 , Training Loss: 0.3905664086341858 Test Loss: 0.39212170243263245\n",
            "Epoch:  110 , Training Loss: 0.3880162537097931 Test Loss: 0.3896069824695587\n",
            "Epoch:  111 , Training Loss: 0.3854884207248688 Test Loss: 0.3871157765388489\n",
            "Epoch:  112 , Training Loss: 0.3829716145992279 Test Loss: 0.38464850187301636\n",
            "Epoch:  113 , Training Loss: 0.3804773688316345 Test Loss: 0.3821982741355896\n",
            "Epoch:  114 , Training Loss: 0.37800347805023193 Test Loss: 0.37977004051208496\n",
            "Epoch:  115 , Training Loss: 0.3755476474761963 Test Loss: 0.3773645758628845\n",
            "Epoch:  116 , Training Loss: 0.37310928106307983 Test Loss: 0.3749793767929077\n",
            "Epoch:  117 , Training Loss: 0.37069305777549744 Test Loss: 0.37261396646499634\n",
            "Epoch:  118 , Training Loss: 0.36829042434692383 Test Loss: 0.37026605010032654\n",
            "Epoch:  119 , Training Loss: 0.3659065067768097 Test Loss: 0.3679395020008087\n",
            "Epoch:  120 , Training Loss: 0.3635420501232147 Test Loss: 0.3656332492828369\n",
            "Epoch:  121 , Training Loss: 0.3611988127231598 Test Loss: 0.3633403778076172\n",
            "Epoch:  122 , Training Loss: 0.3588750958442688 Test Loss: 0.3610658347606659\n",
            "Epoch:  123 , Training Loss: 0.35656869411468506 Test Loss: 0.3588072955608368\n",
            "Epoch:  124 , Training Loss: 0.3542802929878235 Test Loss: 0.35656973719596863\n",
            "Epoch:  125 , Training Loss: 0.35201144218444824 Test Loss: 0.3543473184108734\n",
            "Epoch:  126 , Training Loss: 0.3497609496116638 Test Loss: 0.3521403670310974\n",
            "Epoch:  127 , Training Loss: 0.3475286364555359 Test Loss: 0.3499507009983063\n",
            "Epoch:  128 , Training Loss: 0.3453097939491272 Test Loss: 0.34777647256851196\n",
            "Epoch:  129 , Training Loss: 0.3431136906147003 Test Loss: 0.34561586380004883\n",
            "Epoch:  130 , Training Loss: 0.3409291207790375 Test Loss: 0.34347283840179443\n",
            "Epoch:  131 , Training Loss: 0.33876833319664 Test Loss: 0.34134605526924133\n",
            "Epoch:  132 , Training Loss: 0.33662641048431396 Test Loss: 0.33923783898353577\n",
            "Epoch:  133 , Training Loss: 0.3345049321651459 Test Loss: 0.33714669942855835\n",
            "Epoch:  134 , Training Loss: 0.3324006497859955 Test Loss: 0.3350740969181061\n",
            "Epoch:  135 , Training Loss: 0.33031564950942993 Test Loss: 0.3330220878124237\n",
            "Epoch:  136 , Training Loss: 0.3282454013824463 Test Loss: 0.3309870958328247\n",
            "Epoch:  137 , Training Loss: 0.3261954188346863 Test Loss: 0.3289716839790344\n",
            "Epoch:  138 , Training Loss: 0.32416272163391113 Test Loss: 0.3269776701927185\n",
            "Epoch:  139 , Training Loss: 0.32214826345443726 Test Loss: 0.32500237226486206\n",
            "Epoch:  140 , Training Loss: 0.32015231251716614 Test Loss: 0.32304617762565613\n",
            "Epoch:  141 , Training Loss: 0.3181701600551605 Test Loss: 0.32111281156539917\n",
            "Epoch:  142 , Training Loss: 0.3162005543708801 Test Loss: 0.3191927671432495\n",
            "Epoch:  143 , Training Loss: 0.3142475485801697 Test Loss: 0.3172875642776489\n",
            "Epoch:  144 , Training Loss: 0.31231439113616943 Test Loss: 0.315402090549469\n",
            "Epoch:  145 , Training Loss: 0.3103940784931183 Test Loss: 0.31353211402893066\n",
            "Epoch:  146 , Training Loss: 0.30848583579063416 Test Loss: 0.31168076395988464\n",
            "Epoch:  147 , Training Loss: 0.3066021203994751 Test Loss: 0.30984801054000854\n",
            "Epoch:  148 , Training Loss: 0.3047248423099518 Test Loss: 0.308028906583786\n",
            "Epoch:  149 , Training Loss: 0.3028663396835327 Test Loss: 0.3062297999858856\n",
            "Epoch:  150 , Training Loss: 0.30102869868278503 Test Loss: 0.3044435977935791\n",
            "Epoch:  151 , Training Loss: 0.29920026659965515 Test Loss: 0.302675724029541\n",
            "Epoch:  152 , Training Loss: 0.2973901331424713 Test Loss: 0.300922691822052\n",
            "Epoch:  153 , Training Loss: 0.29559436440467834 Test Loss: 0.29918432235717773\n",
            "Epoch:  154 , Training Loss: 0.29381513595581055 Test Loss: 0.29745903611183167\n",
            "Epoch:  155 , Training Loss: 0.2920500934123993 Test Loss: 0.29574692249298096\n",
            "Epoch:  156 , Training Loss: 0.2903042733669281 Test Loss: 0.29405105113983154\n",
            "Epoch:  157 , Training Loss: 0.28857481479644775 Test Loss: 0.29236748814582825\n",
            "Epoch:  158 , Training Loss: 0.286858469247818 Test Loss: 0.2906981408596039\n",
            "Epoch:  159 , Training Loss: 0.28515952825546265 Test Loss: 0.2890397608280182\n",
            "Epoch:  160 , Training Loss: 0.2834750711917877 Test Loss: 0.28739771246910095\n",
            "Epoch:  161 , Training Loss: 0.2818096876144409 Test Loss: 0.28577178716659546\n",
            "Epoch:  162 , Training Loss: 0.2801609933376312 Test Loss: 0.2841632664203644\n",
            "Epoch:  163 , Training Loss: 0.2785319685935974 Test Loss: 0.2825697958469391\n",
            "Epoch:  164 , Training Loss: 0.27691125869750977 Test Loss: 0.28099602460861206\n",
            "Epoch:  165 , Training Loss: 0.2753108739852905 Test Loss: 0.2794401943683624\n",
            "Epoch:  166 , Training Loss: 0.2737254500389099 Test Loss: 0.277900755405426\n",
            "Epoch:  167 , Training Loss: 0.2721603810787201 Test Loss: 0.2763771414756775\n",
            "Epoch:  168 , Training Loss: 0.2706097960472107 Test Loss: 0.27487123012542725\n",
            "Epoch:  169 , Training Loss: 0.26907655596733093 Test Loss: 0.27337995171546936\n",
            "Epoch:  170 , Training Loss: 0.26755693554878235 Test Loss: 0.27190789580345154\n",
            "Epoch:  171 , Training Loss: 0.2660548686981201 Test Loss: 0.27045533061027527\n",
            "Epoch:  172 , Training Loss: 0.264568030834198 Test Loss: 0.2690163850784302\n",
            "Epoch:  173 , Training Loss: 0.26309704780578613 Test Loss: 0.26759085059165955\n",
            "Epoch:  174 , Training Loss: 0.26163631677627563 Test Loss: 0.2661804258823395\n",
            "Epoch:  175 , Training Loss: 0.26019254326820374 Test Loss: 0.26478415727615356\n",
            "Epoch:  176 , Training Loss: 0.2587631642818451 Test Loss: 0.2634032666683197\n",
            "Epoch:  177 , Training Loss: 0.2573499083518982 Test Loss: 0.26203858852386475\n",
            "Epoch:  178 , Training Loss: 0.25595128536224365 Test Loss: 0.26068657636642456\n",
            "Epoch:  179 , Training Loss: 0.2545657753944397 Test Loss: 0.25935158133506775\n",
            "Epoch:  180 , Training Loss: 0.25319164991378784 Test Loss: 0.2580345571041107\n",
            "Epoch:  181 , Training Loss: 0.2518366277217865 Test Loss: 0.2567305862903595\n",
            "Epoch:  182 , Training Loss: 0.25049495697021484 Test Loss: 0.2554437220096588\n",
            "Epoch:  183 , Training Loss: 0.24916288256645203 Test Loss: 0.25417181849479675\n",
            "Epoch:  184 , Training Loss: 0.24784141778945923 Test Loss: 0.2529146671295166\n",
            "Epoch:  185 , Training Loss: 0.2465360164642334 Test Loss: 0.25167158246040344\n",
            "Epoch:  186 , Training Loss: 0.2452392578125 Test Loss: 0.2504396438598633\n",
            "Epoch:  187 , Training Loss: 0.24395720660686493 Test Loss: 0.2492106407880783\n",
            "Epoch:  188 , Training Loss: 0.2426859587430954 Test Loss: 0.24798943102359772\n",
            "Epoch:  189 , Training Loss: 0.24142548441886902 Test Loss: 0.24677114188671112\n",
            "Epoch:  190 , Training Loss: 0.24017751216888428 Test Loss: 0.245557501912117\n",
            "Epoch:  191 , Training Loss: 0.23894068598747253 Test Loss: 0.24435140192508698\n",
            "Epoch:  192 , Training Loss: 0.23771323263645172 Test Loss: 0.24314802885055542\n",
            "Epoch:  193 , Training Loss: 0.23649823665618896 Test Loss: 0.2419535368680954\n",
            "Epoch:  194 , Training Loss: 0.23529338836669922 Test Loss: 0.24076510965824127\n",
            "Epoch:  195 , Training Loss: 0.2341006100177765 Test Loss: 0.2395881712436676\n",
            "Epoch:  196 , Training Loss: 0.23291753232479095 Test Loss: 0.23842333257198334\n",
            "Epoch:  197 , Training Loss: 0.2317502200603485 Test Loss: 0.23727254569530487\n",
            "Epoch:  198 , Training Loss: 0.2305901199579239 Test Loss: 0.23613585531711578\n",
            "Epoch:  199 , Training Loss: 0.2294372320175171 Test Loss: 0.23501192033290863\n",
            "Epoch:  200 , Training Loss: 0.22830064594745636 Test Loss: 0.233906552195549\n",
            "Epoch:  201 , Training Loss: 0.2271726280450821 Test Loss: 0.23281261324882507\n",
            "Epoch:  202 , Training Loss: 0.2260560840368271 Test Loss: 0.231734037399292\n",
            "Epoch:  203 , Training Loss: 0.22494830191135406 Test Loss: 0.23067212104797363\n",
            "Epoch:  204 , Training Loss: 0.2238515168428421 Test Loss: 0.22962254285812378\n",
            "Epoch:  205 , Training Loss: 0.2227673977613449 Test Loss: 0.22858279943466187\n",
            "Epoch:  206 , Training Loss: 0.22169099748134613 Test Loss: 0.22754983603954315\n",
            "Epoch:  207 , Training Loss: 0.22062478959560394 Test Loss: 0.22652386128902435\n",
            "Epoch:  208 , Training Loss: 0.2195691466331482 Test Loss: 0.22550585865974426\n",
            "Epoch:  209 , Training Loss: 0.2185213416814804 Test Loss: 0.22449758648872375\n",
            "Epoch:  210 , Training Loss: 0.21748675405979156 Test Loss: 0.2234964519739151\n",
            "Epoch:  211 , Training Loss: 0.21645785868167877 Test Loss: 0.2225027233362198\n",
            "Epoch:  212 , Training Loss: 0.2154431790113449 Test Loss: 0.22151516377925873\n",
            "Epoch:  213 , Training Loss: 0.21443380415439606 Test Loss: 0.22054186463356018\n",
            "Epoch:  214 , Training Loss: 0.21343572437763214 Test Loss: 0.21958273649215698\n",
            "Epoch:  215 , Training Loss: 0.2124459147453308 Test Loss: 0.2186415046453476\n",
            "Epoch:  216 , Training Loss: 0.21146546304225922 Test Loss: 0.2177155464887619\n",
            "Epoch:  217 , Training Loss: 0.21049842238426208 Test Loss: 0.21680302917957306\n",
            "Epoch:  218 , Training Loss: 0.20953728258609772 Test Loss: 0.21590213477611542\n",
            "Epoch:  219 , Training Loss: 0.20858442783355713 Test Loss: 0.2150118499994278\n",
            "Epoch:  220 , Training Loss: 0.20764321088790894 Test Loss: 0.21413365006446838\n",
            "Epoch:  221 , Training Loss: 0.20670771598815918 Test Loss: 0.21326307952404022\n",
            "Epoch:  222 , Training Loss: 0.20578433573246002 Test Loss: 0.21240560710430145\n",
            "Epoch:  223 , Training Loss: 0.20486414432525635 Test Loss: 0.21155908703804016\n",
            "Epoch:  224 , Training Loss: 0.2039579600095749 Test Loss: 0.21069298684597015\n",
            "Epoch:  225 , Training Loss: 0.20305797457695007 Test Loss: 0.2098327875137329\n",
            "Epoch:  226 , Training Loss: 0.20216484367847443 Test Loss: 0.20898979902267456\n",
            "Epoch:  227 , Training Loss: 0.2012825906276703 Test Loss: 0.2081618458032608\n",
            "Epoch:  228 , Training Loss: 0.20040662586688995 Test Loss: 0.20734892785549164\n",
            "Epoch:  229 , Training Loss: 0.1995413899421692 Test Loss: 0.20654970407485962\n",
            "Epoch:  230 , Training Loss: 0.19868220388889313 Test Loss: 0.20576100051403046\n",
            "Epoch:  231 , Training Loss: 0.19783243536949158 Test Loss: 0.2049569934606552\n",
            "Epoch:  232 , Training Loss: 0.19699205458164215 Test Loss: 0.20415416359901428\n",
            "Epoch:  233 , Training Loss: 0.19615690410137177 Test Loss: 0.20333795249462128\n",
            "Epoch:  234 , Training Loss: 0.19533149898052216 Test Loss: 0.20252342522144318\n",
            "Epoch:  235 , Training Loss: 0.19451388716697693 Test Loss: 0.20171400904655457\n",
            "Epoch:  236 , Training Loss: 0.19370247423648834 Test Loss: 0.2009129673242569\n",
            "Epoch:  237 , Training Loss: 0.19290082156658173 Test Loss: 0.20012560486793518\n",
            "Epoch:  238 , Training Loss: 0.19210471212863922 Test Loss: 0.19934673607349396\n",
            "Epoch:  239 , Training Loss: 0.19131623208522797 Test Loss: 0.1985788345336914\n",
            "Epoch:  240 , Training Loss: 0.19053688645362854 Test Loss: 0.19781890511512756\n",
            "Epoch:  241 , Training Loss: 0.18976114690303802 Test Loss: 0.19706913828849792\n",
            "Epoch:  242 , Training Loss: 0.1889941394329071 Test Loss: 0.19632183015346527\n",
            "Epoch:  243 , Training Loss: 0.1882351189851761 Test Loss: 0.19558154046535492\n",
            "Epoch:  244 , Training Loss: 0.18748024106025696 Test Loss: 0.19484548270702362\n",
            "Epoch:  245 , Training Loss: 0.18673603236675262 Test Loss: 0.19411982595920563\n",
            "Epoch:  246 , Training Loss: 0.18599598109722137 Test Loss: 0.1934022754430771\n",
            "Epoch:  247 , Training Loss: 0.18526269495487213 Test Loss: 0.19269728660583496\n",
            "Epoch:  248 , Training Loss: 0.18453438580036163 Test Loss: 0.19199930131435394\n",
            "Epoch:  249 , Training Loss: 0.18381279706954956 Test Loss: 0.1913089007139206\n",
            "Epoch:  250 , Training Loss: 0.18309763073921204 Test Loss: 0.19062575697898865\n",
            "Epoch:  251 , Training Loss: 0.18238699436187744 Test Loss: 0.1899481862783432\n",
            "Epoch:  252 , Training Loss: 0.1816832721233368 Test Loss: 0.18927836418151855\n",
            "Epoch:  253 , Training Loss: 0.18098655343055725 Test Loss: 0.18861618638038635\n",
            "Epoch:  254 , Training Loss: 0.18029330670833588 Test Loss: 0.18793074786663055\n",
            "Epoch:  255 , Training Loss: 0.179606094956398 Test Loss: 0.18725910782814026\n",
            "Epoch:  256 , Training Loss: 0.17892485857009888 Test Loss: 0.1866043210029602\n",
            "Epoch:  257 , Training Loss: 0.17824818193912506 Test Loss: 0.18596424162387848\n",
            "Epoch:  258 , Training Loss: 0.17757810652256012 Test Loss: 0.1853397637605667\n",
            "Epoch:  259 , Training Loss: 0.17691445350646973 Test Loss: 0.18472905457019806\n",
            "Epoch:  260 , Training Loss: 0.17625340819358826 Test Loss: 0.1841275990009308\n",
            "Epoch:  261 , Training Loss: 0.17560100555419922 Test Loss: 0.18353226780891418\n",
            "Epoch:  262 , Training Loss: 0.17495261132717133 Test Loss: 0.18294161558151245\n",
            "Epoch:  263 , Training Loss: 0.1743101328611374 Test Loss: 0.18235516548156738\n",
            "Epoch:  264 , Training Loss: 0.17367245256900787 Test Loss: 0.18177081644535065\n",
            "Epoch:  265 , Training Loss: 0.17303800582885742 Test Loss: 0.18119004368782043\n",
            "Epoch:  266 , Training Loss: 0.1724107414484024 Test Loss: 0.18061156570911407\n",
            "Epoch:  267 , Training Loss: 0.17178954184055328 Test Loss: 0.180037260055542\n",
            "Epoch:  268 , Training Loss: 0.17116785049438477 Test Loss: 0.17946593463420868\n",
            "Epoch:  269 , Training Loss: 0.1705554723739624 Test Loss: 0.17889894545078278\n",
            "Epoch:  270 , Training Loss: 0.16994696855545044 Test Loss: 0.17833755910396576\n",
            "Epoch:  271 , Training Loss: 0.16934335231781006 Test Loss: 0.17778481543064117\n",
            "Epoch:  272 , Training Loss: 0.16874024271965027 Test Loss: 0.17723900079727173\n",
            "Epoch:  273 , Training Loss: 0.16814526915550232 Test Loss: 0.17669729888439178\n",
            "Epoch:  274 , Training Loss: 0.16755174100399017 Test Loss: 0.17615917325019836\n",
            "Epoch:  275 , Training Loss: 0.1669638305902481 Test Loss: 0.175627663731575\n",
            "Epoch:  276 , Training Loss: 0.16637764871120453 Test Loss: 0.1751064509153366\n",
            "Epoch:  277 , Training Loss: 0.16579732298851013 Test Loss: 0.1745939403772354\n",
            "Epoch:  278 , Training Loss: 0.16521744430065155 Test Loss: 0.1740862876176834\n",
            "Epoch:  279 , Training Loss: 0.16464419662952423 Test Loss: 0.17358484864234924\n",
            "Epoch:  280 , Training Loss: 0.16407279670238495 Test Loss: 0.17308539152145386\n",
            "Epoch:  281 , Training Loss: 0.16350513696670532 Test Loss: 0.17258600890636444\n",
            "Epoch:  282 , Training Loss: 0.16293874382972717 Test Loss: 0.17208969593048096\n",
            "Epoch:  283 , Training Loss: 0.1623784601688385 Test Loss: 0.17159506678581238\n",
            "Epoch:  284 , Training Loss: 0.1618175208568573 Test Loss: 0.17110265791416168\n",
            "Epoch:  285 , Training Loss: 0.16126137971878052 Test Loss: 0.17061495780944824\n",
            "Epoch:  286 , Training Loss: 0.1607080101966858 Test Loss: 0.17013156414031982\n",
            "Epoch:  287 , Training Loss: 0.16015708446502686 Test Loss: 0.16964755952358246\n",
            "Epoch:  288 , Training Loss: 0.15961101651191711 Test Loss: 0.16916629672050476\n",
            "Epoch:  289 , Training Loss: 0.1590641587972641 Test Loss: 0.1686851978302002\n",
            "Epoch:  290 , Training Loss: 0.15852220356464386 Test Loss: 0.16820405423641205\n",
            "Epoch:  291 , Training Loss: 0.15798045694828033 Test Loss: 0.16772253811359406\n",
            "Epoch:  292 , Training Loss: 0.1574406623840332 Test Loss: 0.16723883152008057\n",
            "Epoch:  293 , Training Loss: 0.15690010786056519 Test Loss: 0.16675473749637604\n",
            "Epoch:  294 , Training Loss: 0.1563614159822464 Test Loss: 0.1662694811820984\n",
            "Epoch:  295 , Training Loss: 0.15582171082496643 Test Loss: 0.16578508913516998\n",
            "Epoch:  296 , Training Loss: 0.15528076887130737 Test Loss: 0.16525988280773163\n",
            "Epoch:  297 , Training Loss: 0.15474019944667816 Test Loss: 0.16474296152591705\n",
            "Epoch:  298 , Training Loss: 0.15419787168502808 Test Loss: 0.16423489153385162\n",
            "Epoch:  299 , Training Loss: 0.15365585684776306 Test Loss: 0.16373255848884583\n",
            "Epoch:  300 , Training Loss: 0.15311506390571594 Test Loss: 0.1632368266582489\n",
            "Epoch:  301 , Training Loss: 0.15257805585861206 Test Loss: 0.16274985671043396\n",
            "Epoch:  302 , Training Loss: 0.15204107761383057 Test Loss: 0.16227704286575317\n",
            "Epoch:  303 , Training Loss: 0.15150713920593262 Test Loss: 0.16180817782878876\n",
            "Epoch:  304 , Training Loss: 0.15097995102405548 Test Loss: 0.16134034097194672\n",
            "Epoch:  305 , Training Loss: 0.1504523903131485 Test Loss: 0.16087648272514343\n",
            "Epoch:  306 , Training Loss: 0.14992739260196686 Test Loss: 0.16041404008865356\n",
            "Epoch:  307 , Training Loss: 0.14940504729747772 Test Loss: 0.15995566546916962\n",
            "Epoch:  308 , Training Loss: 0.14888335764408112 Test Loss: 0.1594938039779663\n",
            "Epoch:  309 , Training Loss: 0.14836546778678894 Test Loss: 0.15902602672576904\n",
            "Epoch:  310 , Training Loss: 0.14784842729568481 Test Loss: 0.15856269001960754\n",
            "Epoch:  311 , Training Loss: 0.14733418822288513 Test Loss: 0.15810750424861908\n",
            "Epoch:  312 , Training Loss: 0.14682258665561676 Test Loss: 0.15766730904579163\n",
            "Epoch:  313 , Training Loss: 0.14631369709968567 Test Loss: 0.1572353094816208\n",
            "Epoch:  314 , Training Loss: 0.14580830931663513 Test Loss: 0.15681181848049164\n",
            "Epoch:  315 , Training Loss: 0.1453046351671219 Test Loss: 0.15640227496623993\n",
            "Epoch:  316 , Training Loss: 0.14480559527873993 Test Loss: 0.1560094952583313\n",
            "Epoch:  317 , Training Loss: 0.14430773258209229 Test Loss: 0.15562190115451813\n",
            "Epoch:  318 , Training Loss: 0.14381447434425354 Test Loss: 0.15523916482925415\n",
            "Epoch:  319 , Training Loss: 0.14332391321659088 Test Loss: 0.15486279129981995\n",
            "Epoch:  320 , Training Loss: 0.14283840358257294 Test Loss: 0.15449044108390808\n",
            "Epoch:  321 , Training Loss: 0.1423521190881729 Test Loss: 0.1541169434785843\n",
            "Epoch:  322 , Training Loss: 0.1418691873550415 Test Loss: 0.15372240543365479\n",
            "Epoch:  323 , Training Loss: 0.14138662815093994 Test Loss: 0.15330755710601807\n",
            "Epoch:  324 , Training Loss: 0.14090430736541748 Test Loss: 0.15287938714027405\n",
            "Epoch:  325 , Training Loss: 0.14042562246322632 Test Loss: 0.1524534523487091\n",
            "Epoch:  326 , Training Loss: 0.13994711637496948 Test Loss: 0.15202319622039795\n",
            "Epoch:  327 , Training Loss: 0.13947319984436035 Test Loss: 0.15153609216213226\n",
            "Epoch:  328 , Training Loss: 0.13899922370910645 Test Loss: 0.15104463696479797\n",
            "Epoch:  329 , Training Loss: 0.1385251134634018 Test Loss: 0.15055890381336212\n",
            "Epoch:  330 , Training Loss: 0.13805314898490906 Test Loss: 0.15007296204566956\n",
            "Epoch:  331 , Training Loss: 0.1375773847103119 Test Loss: 0.14957790076732635\n",
            "Epoch:  332 , Training Loss: 0.13710175454616547 Test Loss: 0.149088054895401\n",
            "Epoch:  333 , Training Loss: 0.13662947714328766 Test Loss: 0.14860959351062775\n",
            "Epoch:  334 , Training Loss: 0.1361618936061859 Test Loss: 0.14816907048225403\n",
            "Epoch:  335 , Training Loss: 0.13569921255111694 Test Loss: 0.14774902164936066\n",
            "Epoch:  336 , Training Loss: 0.13524501025676727 Test Loss: 0.1473485827445984\n",
            "Epoch:  337 , Training Loss: 0.13479337096214294 Test Loss: 0.14696469902992249\n",
            "Epoch:  338 , Training Loss: 0.1343468278646469 Test Loss: 0.146589457988739\n",
            "Epoch:  339 , Training Loss: 0.13389654457569122 Test Loss: 0.14622166752815247\n",
            "Epoch:  340 , Training Loss: 0.13344748318195343 Test Loss: 0.14586199820041656\n",
            "Epoch:  341 , Training Loss: 0.1330016702413559 Test Loss: 0.14550256729125977\n",
            "Epoch:  342 , Training Loss: 0.13255496323108673 Test Loss: 0.14515431225299835\n",
            "Epoch:  343 , Training Loss: 0.1321113556623459 Test Loss: 0.14480726420879364\n",
            "Epoch:  344 , Training Loss: 0.13167116045951843 Test Loss: 0.14444802701473236\n",
            "Epoch:  345 , Training Loss: 0.13123109936714172 Test Loss: 0.14408054947853088\n",
            "Epoch:  346 , Training Loss: 0.13079535961151123 Test Loss: 0.14371700584888458\n",
            "Epoch:  347 , Training Loss: 0.13035807013511658 Test Loss: 0.14335554838180542\n",
            "Epoch:  348 , Training Loss: 0.12992455065250397 Test Loss: 0.1429121494293213\n",
            "Epoch:  349 , Training Loss: 0.12948763370513916 Test Loss: 0.14249074459075928\n",
            "Epoch:  350 , Training Loss: 0.12905387580394745 Test Loss: 0.1420895755290985\n",
            "Epoch:  351 , Training Loss: 0.12861892580986023 Test Loss: 0.1417129635810852\n",
            "Epoch:  352 , Training Loss: 0.1281851977109909 Test Loss: 0.14132830500602722\n",
            "Epoch:  353 , Training Loss: 0.12775078415870667 Test Loss: 0.14093899726867676\n",
            "Epoch:  354 , Training Loss: 0.12731891870498657 Test Loss: 0.14055714011192322\n",
            "Epoch:  355 , Training Loss: 0.12688840925693512 Test Loss: 0.14018692076206207\n",
            "Epoch:  356 , Training Loss: 0.12645643949508667 Test Loss: 0.13979600369930267\n",
            "Epoch:  357 , Training Loss: 0.12602336704730988 Test Loss: 0.13939249515533447\n",
            "Epoch:  358 , Training Loss: 0.1255892515182495 Test Loss: 0.13899436593055725\n",
            "Epoch:  359 , Training Loss: 0.1251583844423294 Test Loss: 0.1386379599571228\n",
            "Epoch:  360 , Training Loss: 0.12472876906394958 Test Loss: 0.13819974660873413\n",
            "Epoch:  361 , Training Loss: 0.12429247796535492 Test Loss: 0.13778913021087646\n",
            "Epoch:  362 , Training Loss: 0.1238596960902214 Test Loss: 0.137425035238266\n",
            "Epoch:  363 , Training Loss: 0.12342768907546997 Test Loss: 0.13708209991455078\n",
            "Epoch:  364 , Training Loss: 0.12299051135778427 Test Loss: 0.13675938546657562\n",
            "Epoch:  365 , Training Loss: 0.12255492806434631 Test Loss: 0.13646866381168365\n",
            "Epoch:  366 , Training Loss: 0.12211762368679047 Test Loss: 0.13619659841060638\n",
            "Epoch:  367 , Training Loss: 0.12168117612600327 Test Loss: 0.1359192132949829\n",
            "Epoch:  368 , Training Loss: 0.12124163657426834 Test Loss: 0.1356068253517151\n",
            "Epoch:  369 , Training Loss: 0.12080223113298416 Test Loss: 0.13527695834636688\n",
            "Epoch:  370 , Training Loss: 0.12036281824111938 Test Loss: 0.13493476808071136\n",
            "Epoch:  371 , Training Loss: 0.11992368847131729 Test Loss: 0.13459651172161102\n",
            "Epoch:  372 , Training Loss: 0.11948588490486145 Test Loss: 0.13425560295581818\n",
            "Epoch:  373 , Training Loss: 0.11904982477426529 Test Loss: 0.13391856849193573\n",
            "Epoch:  374 , Training Loss: 0.11861234158277512 Test Loss: 0.13359367847442627\n",
            "Epoch:  375 , Training Loss: 0.11817627400159836 Test Loss: 0.13327044248580933\n",
            "Epoch:  376 , Training Loss: 0.11774127185344696 Test Loss: 0.1329248994588852\n",
            "Epoch:  377 , Training Loss: 0.11730486154556274 Test Loss: 0.1325748860836029\n",
            "Epoch:  378 , Training Loss: 0.11687181890010834 Test Loss: 0.13220494985580444\n",
            "Epoch:  379 , Training Loss: 0.11643761396408081 Test Loss: 0.13180406391620636\n",
            "Epoch:  380 , Training Loss: 0.11600612848997116 Test Loss: 0.13140617311000824\n",
            "Epoch:  381 , Training Loss: 0.1155741885304451 Test Loss: 0.13103601336479187\n",
            "Epoch:  382 , Training Loss: 0.11514079570770264 Test Loss: 0.13066105544567108\n",
            "Epoch:  383 , Training Loss: 0.11471490561962128 Test Loss: 0.13030149042606354\n",
            "Epoch:  384 , Training Loss: 0.11428318172693253 Test Loss: 0.12995348870754242\n",
            "Epoch:  385 , Training Loss: 0.11385692656040192 Test Loss: 0.12959210574626923\n",
            "Epoch:  386 , Training Loss: 0.11343316733837128 Test Loss: 0.12923811376094818\n",
            "Epoch:  387 , Training Loss: 0.11301019042730331 Test Loss: 0.1289166361093521\n",
            "Epoch:  388 , Training Loss: 0.11258576065301895 Test Loss: 0.12861692905426025\n",
            "Epoch:  389 , Training Loss: 0.11216557770967484 Test Loss: 0.1283148229122162\n",
            "Epoch:  390 , Training Loss: 0.11174676567316055 Test Loss: 0.12800797820091248\n",
            "Epoch:  391 , Training Loss: 0.11133503913879395 Test Loss: 0.1276586949825287\n",
            "Epoch:  392 , Training Loss: 0.11092013120651245 Test Loss: 0.12734454870224\n",
            "Epoch:  393 , Training Loss: 0.11051308363676071 Test Loss: 0.12705664336681366\n",
            "Epoch:  394 , Training Loss: 0.11010604351758957 Test Loss: 0.12673556804656982\n",
            "Epoch:  395 , Training Loss: 0.10970406979322433 Test Loss: 0.12636436522006989\n",
            "Epoch:  396 , Training Loss: 0.10930557548999786 Test Loss: 0.12598802149295807\n",
            "Epoch:  397 , Training Loss: 0.10891145467758179 Test Loss: 0.1257009208202362\n",
            "Epoch:  398 , Training Loss: 0.10851721465587616 Test Loss: 0.1254768669605255\n",
            "Epoch:  399 , Training Loss: 0.10812792927026749 Test Loss: 0.1252116709947586\n",
            "Epoch:  400 , Training Loss: 0.10774657875299454 Test Loss: 0.12488340586423874\n",
            "Epoch:  401 , Training Loss: 0.10736626386642456 Test Loss: 0.12451159954071045\n",
            "Epoch:  402 , Training Loss: 0.10699250549077988 Test Loss: 0.12409303337335587\n",
            "Epoch:  403 , Training Loss: 0.10661352425813675 Test Loss: 0.12364210933446884\n",
            "Epoch:  404 , Training Loss: 0.1062559187412262 Test Loss: 0.12320898473262787\n",
            "Epoch:  405 , Training Loss: 0.10587955266237259 Test Loss: 0.12290674448013306\n",
            "Epoch:  406 , Training Loss: 0.10552866756916046 Test Loss: 0.12257187813520432\n",
            "Epoch:  407 , Training Loss: 0.10516025125980377 Test Loss: 0.12225660681724548\n",
            "Epoch:  408 , Training Loss: 0.10480055958032608 Test Loss: 0.12192705273628235\n",
            "Epoch:  409 , Training Loss: 0.10444071143865585 Test Loss: 0.12157351523637772\n",
            "Epoch:  410 , Training Loss: 0.10409045964479446 Test Loss: 0.12119665741920471\n",
            "Epoch:  411 , Training Loss: 0.10375028848648071 Test Loss: 0.1207822635769844\n",
            "Epoch:  412 , Training Loss: 0.10339059680700302 Test Loss: 0.12040260434150696\n",
            "Epoch:  413 , Training Loss: 0.10305387526750565 Test Loss: 0.12004108726978302\n",
            "Epoch:  414 , Training Loss: 0.10271105915307999 Test Loss: 0.11959685385227203\n",
            "Epoch:  415 , Training Loss: 0.10237432271242142 Test Loss: 0.11917316913604736\n",
            "Epoch:  416 , Training Loss: 0.10203628242015839 Test Loss: 0.11879206448793411\n",
            "Epoch:  417 , Training Loss: 0.10170190036296844 Test Loss: 0.11839189380407333\n",
            "Epoch:  418 , Training Loss: 0.10137225687503815 Test Loss: 0.11799836158752441\n",
            "Epoch:  419 , Training Loss: 0.10103748738765717 Test Loss: 0.11758419126272202\n",
            "Epoch:  420 , Training Loss: 0.10071677714586258 Test Loss: 0.1171301081776619\n",
            "Epoch:  421 , Training Loss: 0.10038737207651138 Test Loss: 0.11667831242084503\n",
            "Epoch:  422 , Training Loss: 0.10006490349769592 Test Loss: 0.11637354642152786\n",
            "Epoch:  423 , Training Loss: 0.09974589198827744 Test Loss: 0.11611334979534149\n",
            "Epoch:  424 , Training Loss: 0.09942632168531418 Test Loss: 0.11576620489358902\n",
            "Epoch:  425 , Training Loss: 0.0991113930940628 Test Loss: 0.1153404489159584\n",
            "Epoch:  426 , Training Loss: 0.0987958088517189 Test Loss: 0.11500880867242813\n",
            "Epoch:  427 , Training Loss: 0.09848426282405853 Test Loss: 0.1146756261587143\n",
            "Epoch:  428 , Training Loss: 0.09817076474428177 Test Loss: 0.1142963245511055\n",
            "Epoch:  429 , Training Loss: 0.09786183387041092 Test Loss: 0.11395832151174545\n",
            "Epoch:  430 , Training Loss: 0.09755217283964157 Test Loss: 0.11365499347448349\n",
            "Epoch:  431 , Training Loss: 0.0972447320818901 Test Loss: 0.11338218301534653\n",
            "Epoch:  432 , Training Loss: 0.09694115817546844 Test Loss: 0.11310291290283203\n",
            "Epoch:  433 , Training Loss: 0.09663626551628113 Test Loss: 0.11281741410493851\n",
            "Epoch:  434 , Training Loss: 0.0963360071182251 Test Loss: 0.11255113035440445\n",
            "Epoch:  435 , Training Loss: 0.09603229910135269 Test Loss: 0.11229918897151947\n",
            "Epoch:  436 , Training Loss: 0.09573028981685638 Test Loss: 0.1120348572731018\n",
            "Epoch:  437 , Training Loss: 0.09543590992689133 Test Loss: 0.11178943514823914\n",
            "Epoch:  438 , Training Loss: 0.0951349139213562 Test Loss: 0.11153814196586609\n",
            "Epoch:  439 , Training Loss: 0.09484298527240753 Test Loss: 0.11130708456039429\n",
            "Epoch:  440 , Training Loss: 0.0945466011762619 Test Loss: 0.11108491569757462\n",
            "Epoch:  441 , Training Loss: 0.09425108134746552 Test Loss: 0.110842764377594\n",
            "Epoch:  442 , Training Loss: 0.09395977109670639 Test Loss: 0.11057859659194946\n",
            "Epoch:  443 , Training Loss: 0.09366758167743683 Test Loss: 0.11033493280410767\n",
            "Epoch:  444 , Training Loss: 0.09337560832500458 Test Loss: 0.11008273810148239\n",
            "Epoch:  445 , Training Loss: 0.093089260160923 Test Loss: 0.10983209311962128\n",
            "Epoch:  446 , Training Loss: 0.09280189871788025 Test Loss: 0.1095445305109024\n",
            "Epoch:  447 , Training Loss: 0.09251434355974197 Test Loss: 0.10927091538906097\n",
            "Epoch:  448 , Training Loss: 0.09222899377346039 Test Loss: 0.10902608186006546\n",
            "Epoch:  449 , Training Loss: 0.09194239974021912 Test Loss: 0.10877659916877747\n",
            "Epoch:  450 , Training Loss: 0.09166305512189865 Test Loss: 0.10848578810691833\n",
            "Epoch:  451 , Training Loss: 0.09138063341379166 Test Loss: 0.1082046702504158\n",
            "Epoch:  452 , Training Loss: 0.09110015630722046 Test Loss: 0.10793405771255493\n",
            "Epoch:  453 , Training Loss: 0.09081796556711197 Test Loss: 0.10764919966459274\n",
            "Epoch:  454 , Training Loss: 0.09053849428892136 Test Loss: 0.10737767070531845\n",
            "Epoch:  455 , Training Loss: 0.09026270359754562 Test Loss: 0.10715765506029129\n",
            "Epoch:  456 , Training Loss: 0.08998594433069229 Test Loss: 0.10687839984893799\n",
            "Epoch:  457 , Training Loss: 0.08971089869737625 Test Loss: 0.10656274110078812\n",
            "Epoch:  458 , Training Loss: 0.08943406492471695 Test Loss: 0.10632305592298508\n",
            "Epoch:  459 , Training Loss: 0.08916282653808594 Test Loss: 0.10616516321897507\n",
            "Epoch:  460 , Training Loss: 0.08888444304466248 Test Loss: 0.10602740943431854\n",
            "Epoch:  461 , Training Loss: 0.08861285448074341 Test Loss: 0.10574416816234589\n",
            "Epoch:  462 , Training Loss: 0.0883331447839737 Test Loss: 0.10550632327795029\n",
            "Epoch:  463 , Training Loss: 0.08806831389665604 Test Loss: 0.10540665686130524\n",
            "Epoch:  464 , Training Loss: 0.08779049664735794 Test Loss: 0.10529350489377975\n",
            "Epoch:  465 , Training Loss: 0.08752746134996414 Test Loss: 0.10511616617441177\n",
            "Epoch:  466 , Training Loss: 0.08726011216640472 Test Loss: 0.10487326979637146\n",
            "Epoch:  467 , Training Loss: 0.08699064701795578 Test Loss: 0.10474331676959991\n",
            "Epoch:  468 , Training Loss: 0.0867290198802948 Test Loss: 0.10465368628501892\n",
            "Epoch:  469 , Training Loss: 0.08645973354578018 Test Loss: 0.10451269149780273\n",
            "Epoch:  470 , Training Loss: 0.08620356023311615 Test Loss: 0.10423544049263\n",
            "Epoch:  471 , Training Loss: 0.08593285828828812 Test Loss: 0.10394316166639328\n",
            "Epoch:  472 , Training Loss: 0.08567380160093307 Test Loss: 0.10379263013601303\n",
            "Epoch:  473 , Training Loss: 0.08540947735309601 Test Loss: 0.10365476459264755\n",
            "Epoch:  474 , Training Loss: 0.08514264225959778 Test Loss: 0.10346526652574539\n",
            "Epoch:  475 , Training Loss: 0.08489153534173965 Test Loss: 0.10327271372079849\n",
            "Epoch:  476 , Training Loss: 0.08461759984493256 Test Loss: 0.10308794677257538\n",
            "Epoch:  477 , Training Loss: 0.08437694609165192 Test Loss: 0.1029757708311081\n",
            "Epoch:  478 , Training Loss: 0.08409623056650162 Test Loss: 0.10281705856323242\n",
            "Epoch:  479 , Training Loss: 0.08384446054697037 Test Loss: 0.10266724973917007\n",
            "Epoch:  480 , Training Loss: 0.08357077836990356 Test Loss: 0.10255833715200424\n",
            "Epoch:  481 , Training Loss: 0.08332624286413193 Test Loss: 0.10243683308362961\n",
            "Epoch:  482 , Training Loss: 0.08305846899747849 Test Loss: 0.10237224400043488\n",
            "Epoch:  483 , Training Loss: 0.0828094631433487 Test Loss: 0.10224205255508423\n",
            "Epoch:  484 , Training Loss: 0.08254914730787277 Test Loss: 0.1020255759358406\n",
            "Epoch:  485 , Training Loss: 0.0823049247264862 Test Loss: 0.10182676464319229\n",
            "Epoch:  486 , Training Loss: 0.0820503979921341 Test Loss: 0.1015610471367836\n",
            "Epoch:  487 , Training Loss: 0.08180048316717148 Test Loss: 0.10137366503477097\n",
            "Epoch:  488 , Training Loss: 0.08155163377523422 Test Loss: 0.10125379264354706\n",
            "Epoch:  489 , Training Loss: 0.08131008595228195 Test Loss: 0.10111071914434433\n",
            "Epoch:  490 , Training Loss: 0.08106277137994766 Test Loss: 0.10099483281373978\n",
            "Epoch:  491 , Training Loss: 0.08081407845020294 Test Loss: 0.1009058877825737\n",
            "Epoch:  492 , Training Loss: 0.08057320863008499 Test Loss: 0.10079590231180191\n",
            "Epoch:  493 , Training Loss: 0.08033009618520737 Test Loss: 0.10073325783014297\n",
            "Epoch:  494 , Training Loss: 0.08008921146392822 Test Loss: 0.10061059892177582\n",
            "Epoch:  495 , Training Loss: 0.07984469830989838 Test Loss: 0.10046259313821793\n",
            "Epoch:  496 , Training Loss: 0.07960634678602219 Test Loss: 0.10031667351722717\n",
            "Epoch:  497 , Training Loss: 0.07936559617519379 Test Loss: 0.10009012371301651\n",
            "Epoch:  498 , Training Loss: 0.07912280410528183 Test Loss: 0.0999022126197815\n",
            "Epoch:  499 , Training Loss: 0.07887832075357437 Test Loss: 0.09970597922801971\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8VFX6x/HPk15JpyaQgCC9hgAC\nCtgoK6i4LiAKKuLad10b6wqrqyvqLmJdFxUL+kMQy9oBFewCAemdECABUiGkk8mc3x93gCCRhJDk\nZibP+/WaVzL3npl5TozfHM6991wxxqCUUsqzeNldgFJKqdqn4a6UUh5Iw10ppTyQhrtSSnkgDXel\nlPJAGu5KKeWBNNyVUsoDabgrpZQH0nBXSikP5GPXB0dHR5v4+Hi7Pl4ppdzS6tWrs40xMVW1sy3c\n4+PjSU5OtuvjlVLKLYnInuq002kZpZTyQBruSinlgTTclVLKA9k2566UarjKyspIS0ujpKTE7lIa\nrYCAAGJjY/H19a3R6zXclVKnSEtLIzQ0lPj4eETE7nIaHWMMOTk5pKWlkZCQUKP30GkZpdQpSkpK\niIqK0mC3iYgQFRV1Vv9y0nBXSlVKg91eZ/vzd7tw35VVwMzPt6K3B1RKqd/mduG+bGsmb3yzidd/\nTLW7FKVUHTl8+DAvvvhijV47cuRIDh8+fNo206dP58svv6zR+/9afHw82dnZtfJetcntwv3GgGV8\nFzKNlz/7kV/2HrK7HKVUHThduDscjtO+9rPPPiM8PPy0bR555BEuuuiiGtfnDtwu3KVVH6Ikn1f9\nZnHP2z9xuOio3SUppWrZAw88wK5du+jZsyf33nsvy5cvZ/DgwYwePZrOnTsDcPnll9OnTx+6dOnC\nnDlzjr/22Eg6NTWVTp06cdNNN9GlSxcuueQSiouLAZg8eTKLFi063n7GjBn07t2bbt26sXXrVgCy\nsrK4+OKL6dKlC1OmTKFNmzZVjtBnzZpF165d6dq1K7NnzwagsLCQUaNG0aNHD7p27cqCBQuO97Fz\n5850796de+65p3Z/gLjjqZAteyJjX6XjOxO4v3gWf1kQxcuTkvDy0oM/StWFhz/exOb9R2r1PTu3\nbMKMy7r85v6ZM2eyceNG1q5dC8Dy5ctZs2YNGzduPH5q4Ny5c4mMjKS4uJi+ffsyduxYoqKiTnqf\nHTt2MH/+fF5++WWuvvpq3nvvPSZOnHjK50VHR7NmzRpefPFF/vWvf/HKK6/w8MMPM2zYMKZNm8YX\nX3zBq6++eto+rV69mtdee40VK1ZgjKFfv35ccMEFpKSk0LJlSz799FMA8vLyyMnJ4YMPPmDr1q2I\nSJXTSDXhdiN3ADqORC79J5d4rSJp1zPM/mqH3RUppepYUlLSSed8P/vss/To0YP+/fuzb98+duw4\nNQcSEhLo2bMnAH369CE1NbXS977yyitPafP9998zbtw4AIYPH05ERMRp6/v++++54oorCA4OJiQk\nhCuvvJLvvvuObt26sXTpUu6//36+++47wsLCCAsLIyAggBtvvJH333+foKCgM/1xVMn9Ru7H9L8F\nk7OLm5NfYdry5nza7F5GdW9hd1VKeZzTjbDrU3Bw8PHvly9fzpdffslPP/1EUFAQQ4YMqfSccH9/\n/+Pfe3t7H5+W+a123t7eVc7pn6kOHTqwZs0aPvvsM/72t79x4YUXMn36dFauXMlXX33FokWLeP75\n5/n6669r9XPdc+QOIIKMeILydhfxqO9rvP/um2zan2d3VUqpWhAaGkp+fv5v7s/LyyMiIoKgoCC2\nbt3Kzz//XOs1DBw4kIULFwKwZMkSDh06/QkcgwcP5sMPP6SoqIjCwkI++OADBg8ezP79+wkKCmLi\nxInce++9rFmzhoKCAvLy8hg5ciRPP/0069atq/X63XfkDuDtg/fvX8PxyiU8kz2bP77elNl3TiA6\nxL/q1yqlGqyoqCgGDhxI165dGTFiBKNGjTpp//Dhw3nppZfo1KkT5557Lv3796/1GmbMmMH48eOZ\nN28eAwYMoHnz5oSGhv5m+969ezN58mSSkpIAmDJlCr169WLx4sXce++9eHl54evry3/+8x/y8/MZ\nM2YMJSUlGGOYNWtWrdcvdl0MlJiYaGrtZh2H91H236FkFBkebvocL9w8HD8f9/1HiVJ227JlC506\ndbK7DFuVlpbi7e2Nj48PP/30E7fccsvxA7z1pbL/DiKy2hiTWNVrPSMBw+PwnbiQFt75TM2YwSMf\nrtUrWJVSZ2Xv3r307duXHj16cOedd/Lyyy/bXdIZce9pmYpa9cb7yv/Qd9EN7Fr7MG+1eoZrB8Tb\nXZVSyk21b9+eX375xe4yaswzRu7HdB2Lc9A9jPNZzu5PZ7EiJcfuipRSyhaeFe6A17AHKWs/ggd9\n5vHGW6+RdqjI7pKUUqreeVy44+WF71Uv44jswEznLP7+2kcUHy23uyqllKpXnhfuAP6h+F+7gAA/\nPx44/DAPLfhBD7AqpRoVzwx3gIh4/MbPI8E7k5HbH+Kl5dvtrkgpVU1ns+QvwOzZsykqqnxKdsiQ\nIdTaadgNmOeGO0DCYLxGPskw77V4ffUIX2/NsLsipVQ11GW4NxaeHe6A9L0RR+8buNnnE5bMf47d\n2YV2l6SUqsKvl/wFeOqpp+jbty/du3dnxowZQOXL6T777LPs37+foUOHMnTo0NN+zvz58+nWrRtd\nu3bl/vvvB6C8vJzJkyfTtWtXunXrxtNPPw1YC5UdW6L32IJiDZnnnOd+Gj6jnqQ0YzMz0v/L3W+0\nZ9YdEwn087a7LKXcw+cPwMENtfuezbvBiJm/ufvXS/4uWbKEHTt2sHLlSowxjB49mm+//ZasrKxT\nltMNCwtj1qxZLFu2jOjo6N/8jP3793P//fezevVqIiIiuOSSS/jwww+Ji4sjPT2djRs3Ahxfjnfm\nzJns3r0bf3//Olmit7Z5/MgdAG9f/Me9iVdgOPcfeYx/vv+T3RUppc7AkiVLWLJkCb169aJ3795s\n3bqVHTt2VLqcbnWtWrWKIUOGEBMTg4+PD9dccw3ffvstbdu2JSUlhTvuuIMvvviCJk2aANC9e3eu\nueYa3nrrLXx8Gv64uFoVishw4BnAG3jFGHPKn1wRuRr4O2CAdcaYCbVY59kLbYb/+HnEvTaS8zc9\nxMKVr3N1Uhu7q1Kq4TvNCLu+GGOYNm0aN9988yn7KltO92xERESwbt06Fi9ezEsvvcTChQuZO3cu\nn376Kd9++y0ff/wxjz32GBs2bGjQIV/lyF1EvIEXgBFAZ2C8iHT+VZv2wDRgoDGmC/CnOqj17LXu\nj1zyGBd7ryHt48d0iWClGqhfL/l76aWXMnfuXAoKCgBIT08nMzOz0uV0K3t9ZZKSkvjmm2/Izs6m\nvLyc+fPnc8EFF5CdnY3T6WTs2LE8+uijrFmzBqfTyb59+xg6dChPPPEEeXl5x2tpqKrzZycJ2GmM\nSQEQkXeAMcDmCm1uAl4wxhwCMMZk1nahtcWr/x8p3bOSP21dyL1vdGT6n+4gLNDX7rKUUhX8esnf\np556ii1btjBgwAAAQkJCeOutt9i5c+cpy+kCTJ06leHDh9OyZUuWLVtW6We0aNGCmTNnMnToUIwx\njBo1ijFjxrBu3Tquv/56nE4nAI8//jjl5eVMnDiRvLw8jDHceeedVd6E225VLvkrIlcBw40xU1zP\nrwX6GWNur9DmQ2A7MBBr6ubvxpgvKnmvqcBUgNatW/fZs2dPbfXjzBwtpPjFIRQf2s+Tbebw+PUj\nEdF7sCp1jC752zA0hCV/fYD2wBBgPPCyiJzyZ80YM8cYk2iMSYyJiamlj64Bv2ACJ84nxMdwVerf\nmffDLvtqUUqpOlCdcE8H4io8j3VtqygN+MgYU2aM2Y01im9fOyXWkehz8B3zDIle2ylc8o9av7u7\nUkrZqTrhvgpoLyIJIuIHjAM++lWbD7FG7YhINNABSKnFOuuEdP89JV0ncLPX/5g7by5FR2v3xrhK\nuTNdj8leZ/vzrzLcjTEO4HZgMbAFWGiM2SQij4jIaFezxUCOiGwGlgH3GmPcYjH1gNH/oiSsHfcV\nPc2/3v/O7nKUahACAgLIycnRgLeJMYacnBwCAgJq/B6ecQ/Vs5WxCcdLQ/jRcS6Hx77D6J6xdlek\nlK3KyspIS0ujpKTE7lIarYCAAGJjY/H1PflsvuoeUG24Z+DXp2ZdkBGPc/5nf+HfH/yTvXH/onVU\nkN1VKWUbX19fEhIS7C5DnYXGsfxANXj3vZGic37HnfIOz82bT1m50+6SlFKqxjTcjxEhaOwLlAU1\n587cx3nus9V2V6SUUjWm4V5RYDhB41+npVcubVdO54ed2XZXpJRSNaLh/mtxSTjPv4/LvX9k6fxn\nOVx01O6KlFLqjGm4V8J3yL0UNuvLPY45zFq4RE8HU0q5HQ33ynh5Ezx+Lr4+3lyRMoP3k1Ptrkgp\npc6IhvtvCW+N7+XP0strJ1mfPMLenMZ9P0allHvRcD8Nr25jKez8B26SD5gz700cenqkUspNaLhX\nIXjMvykOjuWWQ0/yytJf7C5HKaWqRcO9Kv6hhIx/g+ZymLgf/sqaPbl2V6SUUlXScK+O2D6UXTCN\nUd4/s+TtWRSU6uqRSqmGTcO9mgIuuJsjzfpxR+kcXlx0yk2mlFKqQdFwry4vb5pMmIuXjz+XbnuI\nxettukWgUkpVg4b7mQiLxfeK5+jhlULa+9M5mKfLoSqlGiYN9zPk0/VyjnSewPXmf7w67w2cTr16\nVSnV8Gi410CTy/9FQUgbbsiayYJv1tpdjlJKnULDvSb8ggmd8DoxcoSo5feRkplvd0VKKXUSDfca\nkla9KB78Vy6RlXwx70m9elUp1aBouJ+F0KF/JiumP5OP/JcFX3xldzlKKXWchvvZ8PIieuJcnN7+\n9Fx5D1vTsuyuSCmlAA33syZhrTCjn6eLpLJx3n0cdej0jFLKfhrutSC05xjS2o3nqtL3+d97b9ld\njlJKabjXltg/zCLDvw3nb36IDdt22l2OUqqRq1a4i8hwEdkmIjtF5IFK9k8WkSwRWet6TKn9Uhs4\nvyCCJ7xOhBSQt/BWinVxMaWUjaoMdxHxBl4ARgCdgfEi0rmSpguMMT1dj1dquU63ENKmN+m972FQ\n+QoWz3/a7nKUUo1YdUbuScBOY0yKMeYo8A4wpm7Lcl8Jv7uP1OAeDNv9b5LXr7e7HKVUI1WdcG8F\n7KvwPM217dfGish6EVkkInG1Up078vKm+XWv4SsGPryNotKjdleklGqEauuA6sdAvDGmO7AUeKOy\nRiIyVUSSRSQ5K8tzzwkPaNaOgwMeItG5nuXzHre7HKVUI1SdcE8HKo7EY13bjjPG5BhjSl1PXwH6\nVPZGxpg5xphEY0xiTExMTep1GwmX3Mb2Jv0Zuu951q9LtrscpVQjU51wXwW0F5EEEfEDxgEfVWwg\nIi0qPB0NbKm9Et2UCLHXvUKZ+OH9v1spKS2t+jVKKVVLqgx3Y4wDuB1YjBXaC40xm0TkEREZ7Wp2\np4hsEpF1wJ3A5Loq2J0ERcdxYOA/6OLcxs/zpttdjlKqERFj7LnZRGJioklObgTTFcawfvaVdDz8\nDSmXf0zHXgPtrkgp5cZEZLUxJrGqdnqFal0TIWHySxyRUPw+voXSkiK7K1JKNQIa7vUgNKIZ+89/\nkrbOPax985QLfJVSqtZpuNeT7sP+wIrwUSSmv8muNV/bXY5SysNpuNejjpOeJ0Oi8f/kNo4WF9hd\njlLKg2m416OwiEj2D/k3sc79bHrzL3aXo5TyYBru9SxxyBiWh4+l14F32Lv6C7vLUUp5KA13G3Sf\nPIs9tMD/0ztwFB22uxyllAfScLdBZHg46UNmEV2exfZ5d9ldjlLKA2m42+S8ISP5MuIPdD7wIekr\n/2d3OUopD6PhbqPek55iJ3EEfvEnHAU5dpejlPIgGu42ioloQvrQpwktz2P3vNvsLkcp5UE03G12\n/vkX8XnkRNpnfM6BnxbYXY5SykNouNtMROg/6TE20ZagJfdQnp9pd0lKKQ+g4d4ANA0PJWPY0wQ4\ni9n3xlSwaaVOpZTn0HBvIIYOvoCPom4gPnsZGT+8aXc5Sik3p+HeQIgIF1w3g184l5CvplF+OM3u\nkpRSbkzDvQFpGh5M1oVPI04H+9+8SadnlFI1puHewFw86Dzej55KXO6PZH8zx+5ylFJuSsO9gRER\nLr7ur/xMN0K+mY4zZ7fdJSml3JCGewPULCyInAtnUeYUMufdCE6n3SUppdyMhnsDNXJQXxZG30rz\nw6vJ+fpZu8tRSrkZDfcGSkQYde29fENvQr5/DEfGVrtLUkq5EQ33Bqx5eCDFw5+myPiR/dYNUO6w\nuySllJvQcG/ghvfvyQct/0zz/E1kfP6E3eUopdyEhrsbuPLaO1nqdR6RybMo3bfW7nKUUm6gWuEu\nIsNFZJuI7BSRB07TbqyIGBFJrL0SVXiQH0GXzybXhHDk7UlQVmx3SUqpBq7KcBcRb+AFYATQGRgv\nIp0raRcK3AWsqO0iFQzsfi6ft5tOTEkqBxbdZ3c5SqkGrjoj9yRgpzEmxRhzFHgHGFNJu38ATwAl\ntVifquDqcZN41/cyWmx7k6JNn9ldjlKqAatOuLcC9lV4nubadpyI9AbijDGfnu6NRGSqiCSLSHJW\nVtYZF9vYBfn5cM74p9jqjKP8/VuhQH+GSqnKnfUBVRHxAmYBf6mqrTFmjjEm0RiTGBMTc7Yf3Sj1\natuCFb2ewM9RQOZbU3RxMaVUpaoT7ulAXIXnsa5tx4QCXYHlIpIK9Ac+0oOqdWfC6BG8ETyZpgeX\nc+T7/9pdjlKqAapOuK8C2otIgoj4AeOAj47tNMbkGWOijTHxxph44GdgtDEmuU4qVvh6ezFs0nS+\nMz3w/3o65Xr1qlLqV6oMd2OMA7gdWAxsARYaYzaJyCMiMrquC1SVO6dZE3IvepoCpx+H3rxWT49U\nSp2kWnPuxpjPjDEdjDHtjDGPubZNN8Z8VEnbITpqrx+jB/VmQau/El24naxFf7a7HKVUA6JXqLox\nEeGaa29ins+VxGybT3Hy23aXpJRqIDTc3VxYoC+dr3mSlc6OeH16NyZT59+VUhruHqFPQgybzptN\nvtOPI29eA0cL7S5JKWUzDXcPcd0l/ZkT/QCh+bs4vOhOPf9dqUZOw91DeHsJUybfyKvevyd8+yKK\nf9KbayvVmGm4e5CmoQH0vPZxvnL2xm/JNEzqD3aXpJSyiYa7h+mbEE3a0NmkOptS/PZEyEuzuySl\nlA003D3QdUO683bCPyk/WkzBm+P0AielGiENdw8kIvx5/GU8EfRnQnI2UPzBXXqAValGRsPdQ4UG\n+HLd5Ft5zvl7AjcvwPHTi3aXpJSqRxruHqxDs1Dajn2YxeWJyJKHMDuW2l2SUqqeaLh7uFE9WrF9\n4L/Z6oyl7J1JkLHJ7pKUUvVAw70RuO2SHrzV9klyHX6UvHEV5GfYXZJSqo5puDcCXl7CQxMu4tEm\nM3AW5lAy72o4WmR3WUqpOqTh3kgE+fnwwA1/4EHvP+GXuY6yRVPB6bS7LKVUHdFwb0RiI4K45ro/\n8nj5RHy3f4xj6XS7S1JK1REN90YmMT6SLlc8wJuOi/H56TmcPz5vd0lKqTqg4d4IXd47lsILH+PT\n8iS8ljwI6xbYXZJSqpZpuDdSfxzSgZW9ZvJjeWecH94Keg68Uh5Fw72REhGmX96b+e1msrk8lvJ3\nJsK+lXaXpZSqJRrujZi3l/DkhEE8FfMYaY4wHPOugoMb7C5LKVULNNwbuUA/b2ZdfzHTgv9B9lEf\nHK+PhozNdpellDpLGu6KqBB/npo6mrv8HuFQicHx+mWQtd3uspRSZ0HDXQHQKjyQmVOv4I/eD5NX\n7KD89d9Bzi67y1JK1VC1wl1EhovINhHZKSIPVLL/jyKyQUTWisj3ItK59ktVdS0hOphHp1zBFKZz\npLCE8tdGQW6K3WUppWqgynAXEW/gBWAE0BkYX0l4/58xppsxpifwJDCr1itV9aJTiyZMv+FKrnc+\nSGFhAeWv6QheKXdUnZF7ErDTGJNijDkKvAOMqdjAGHOkwtNgQG/748Z6tY7gvkljubbsQfIL8nG+\neqkeZFXKzVQn3FsB+yo8T3NtO4mI3CYiu7BG7nfWTnnKLue1i+beSb9ngmMGucXlOF8bCelr7C5L\nKVVNtXZA1RjzgjGmHXA/8LfK2ojIVBFJFpHkrKys2vpoVUcGtY/mb5MuZ5xjBhmlvjjfuAz2/GR3\nWUqpaqhOuKcDcRWex7q2/ZZ3gMsr22GMmWOMSTTGJMbExFS/SmWb886J5h+TL2O84++kOcIw866A\nXV/bXZZSqgrVCfdVQHsRSRARP2Ac8FHFBiLSvsLTUcCO2itR2W1AuyieuH44ExwzSHE2w7x9Nax/\n1+6ylFKnUWW4G2McwO3AYmALsNAYs0lEHhGR0a5mt4vIJhFZC9wNTKqzipUt+rWNYtb1FzOxfAa/\nmA7w/hT4fjYYPXauVEMkxqb/ORMTE01ycrItn61qbmN6HlNe/YGHzXNcan6EpJth+OPg5W13aUo1\nCiKy2hiTWFU7n/ooRnmOrq3C+L9bzue6V/w4UBLJ5JX/hfz9cOXL4Btod3lKKRddfkCdsbYxISy6\ndRD/Fz6Vx8qvw2z5BN64DPIz7C5NKeWi4a5qpHlYAAtvHsDqFuO4tewuyvZvgDlD9Fx4pRoIDXdV\nY+FBfrw9pT/Ojpcxpng6h0vKMa+N0DNplGoANNzVWQn08+Y/1/Rh0OBhXJj/d7Z7t7fOpFk6Hcod\ndpenVKOlB1TVWfPyEv46shPxUcGM+V8I/w55m1E/PGNN0Yx9BUKb212iUo2OhruqNRP6tSYuMpBb\n3/bnZ2nH3/e9ivdLg+GqVyHhfLvLU6pR0WkZVasGt4/h49sHsbLJcEYVP8whE4R5cwx88xQ4y+0u\nT6lGQ8Nd1br46GA+uO082nfrx8Dc6awMHgrLHrVOlzy0x+7ylGoUNNxVnQjy8+HZcT25e1QvJuTe\nyBMBd1G+fx38ZyCsna/LFihVxzTcVZ0REaYMbstbN/bnXcf5XFz8TzKC28OHf4SF10Fhjt0lKuWx\nNNxVnRvQLorP7xpMbNtODDhwN+9F3oTZ9jn8ZwBs+cTu8pTySBruql7EhPrz+uS+TBvZhQcyhjHJ\neyaFvpGw4Bp4dzIU6M1blKpNGu6q3nh5CTed35b3bjmPvX7t6HVwGt/E3ozZ+im8kGRd2apz8UrV\nCg13Ve+6x4bzyZ2DGds3gUk7L2BKwCwKQ9pYV7bOHwdH9ttdolJuT8Nd2SLE34fHr+zOGzcksams\nJT3T72FZ/J8wKd/AC/0gea6eF6/UWdBwV7a6oEMMi/98PmN6xXH91iRuCHyGgsiu8Mmf4eVhsG+V\n3SUq5ZY03JXtwgJ9+dfve/DqpEQ2lUTRPfU23kt4GGf+QXj1IvjwNj3gqtQZ0nBXDcaFnZqx9O4L\nmNCvDfdsbc9Fpf8mpcMUzPoF8Fxv+G4WlBXbXaZSbkHDXTUoYYG+PHp5N96/5Tz8Q8IYtn4YD7aY\nQ3HL/vDVw/BconWFq87HK3VaGu6qQerVOoKPbx/I30Z14sN9QfTaeQMLu75EeXCMdYXrfy+AXV/b\nXaZSDZaGu2qwfLy9mDK4LUvvvoCLOjXjvuQmDMp+kJV9nsKU5sG8K+D130Hq93aXqlSDo+GuGrxW\n4YE8P6E37/5xADFhgVz9Qyuu8nmOfUkPQfZ2eH0UvDYSUpbrRVBKuWi4K7fRNz6SD28dyL9/34O0\nIw4Gf9uJu5q9TtagRyA3Bd4cA3OHw7YvwOm0u1ylbCXGppFOYmKiSU5OtuWzlfsrOurgv9+k8Mp3\nKRSXlXN1zxjua7aayDXPw5E0iO4AA26D7uPAN8DucpWqNSKy2hiTWFW7ao3cRWS4iGwTkZ0i8kAl\n++8Wkc0isl5EvhKRNjUpWqnqCvLz4c8Xd+Db+4Zyw8AE3t+QQ7+lbXi47dvkjXwRfALg47tgdldY\n/oQuL6wanSpH7iLiDWwHLgbSgFXAeGPM5gpthgIrjDFFInILMMQY84fTva+O3FVtOpBXzHNf72Th\nqn34eAvjEuO4ve0BotfPgR1LwCcQeo6H/rdB9Dl2l6tUjVV35F6dcB8A/N0Yc6nr+TQAY8zjv9G+\nF/C8MWbg6d5Xw13VhdTsQl5YtpMPfklHBK7sFcsd3cuJ3fIqrF8A5WVw7gjoOwXaDgUvPeyk3Ett\nhvtVwHBjzBTX82uBfsaY23+j/fPAQWPMo5XsmwpMBWjdunWfPXv0fpqqbqQdKuLlb1N4Z9U+ysqd\njOrekjuSQumw5x1rUbKiHIhsB31vhJ4TIDDC7pKVqhZbwl1EJgK3AxcYY0pP9746clf1ISu/lFe/\n381bP++hoNTBoHOiuaF/S4aU/4jXqlcgbaU1ZdPtKugzGVr1ARG7y1bqN9X7tIyIXAQ8hxXsmVV9\nsIa7qk95RWW8tWIP837aw8EjJSREBzNpQBuujj1E0LrXYcO7UFYE0edaI/ke4yC0ud1lK3WK2gx3\nH6wDqhcC6VgHVCcYYzZVaNMLWIQ1wt9RnQI13JUdysqdfL7xIK/9sJtf9h4m1N+Hq/vGMbFnOAkZ\nS2Ht27BvBYgXnHORFfQdRujplKrBqLVwd73ZSGA24A3MNcY8JiKPAMnGmI9E5EugG3DA9ZK9xpjR\np3tPDXdlt7X7DvPaD7v5dP0BHE5D/7aRjE9qzYgWBfhtXGAtUJa/H/ybwLkjocsV0G4o+PjbXbpq\nxGo13OuChrtqKDLzS1i0Oo13Vu5jb24REUG+jO0dy7jEVpxTkAyb3octn0DJYfAPg06/g46/g7ZD\nwC/I7vJVI6PhrtQZcjoNP+7KYf7KvSzedBCH09CrdThX9o7lsi5RhB/8CTZ9YAV9aZ51ILbdUGtU\n3+FSCGlqdxdUI6DhrtRZyMov5YNf0nhvdTrbMvLx9RaGdWzKFb1iGdo+DP+0n2Hb57DtM8jbBwjE\n9oWOI62wj+6gZ92oOqHhrlQtMMaw+cAR3l+Tzv/W7ie7oJTwIF9+170Fo7q1JCk+Au+sTbD1M9j2\nKRxYZ70wLM6atmk3zPoaFGm8E6FFAAAOjklEQVRfJ5RH0XBXqpY5yp18tzObD9aks2TzQUrKnESH\n+DOia3NGdmtBUkIk3vn7YfsX1o1Edn9nTd8g0KKHNYXTdii07q8HZVWNabgrVYcKSx0s25bJp+sP\nsGxb5klBP6p7C/rGR+JtymH/L1bQpyyDtFXgdIBvELQ5zwr6thdA0y66DIKqNg13pepJZUEfGezH\nsI5NubhzMwa3jybIzwdK8627Ru1aZoV99nbrDQIjIWEwJJwPCUMgqp3O16vfpOGulA2OBf3SzRks\n25rJkRIH/j5eDDonmos7N+PCTs2ICXVNyeSlQ+p3kPIN7P4GjqRb20NbuoL+fGtkHxZrX4dUg6Ph\nrpTNysqdrNqdy5LNGSzdnEH64WJEoFdcOBd3bs6FnZrSvmkIImLdHjA3xQr53d9ajyLXGvSRbU+E\nffz5EBJjb8eUrTTclWpAjDFsOZDP0s0ZLN1ykI3pRwDr/rBDO8Yw9NymDGgXZU3fgHWbwMzNJ4J+\nzw9Qar2Gpp2tg7Jx/SAuCSISdBqnEdFwV6oBO5BXzPJtWSzbmsn3O7MpOlqOn48X/dtGMfRcK+zj\no4NPvKDcAQfWWiP71O8hLflE2AfHnAj6uH7QoqeuhePBNNyVchOljnJW7T7Esm2ZLNuWSUpWIQAJ\n0cEMOTeGYR2bkpQQib+P94kXOcsha6u1yNm+ldYjd5e1z9sPWiVC/CCIHwixSbpMggfRcFfKTe3J\nKbRG9dsy+WlXDqUOJ4G+3gw8J5qhHWMYcm5TWoUHnvrCgizrdMu9P0LqD9ZI3zjByxdiE60raFv2\ngpY9dSrHjWm4K+UBio+W83NKDsu2ZfL11kzSDhUD0L5pCAPPiWbgOdH0axtJkwDfU19ccsQa2ad+\nZ03lHFgPzjJrX0C4dWFVy55W4LfoCRHxGvhuQMNdKQ9jjGFXVgHLtmbx7Y4sVqXmUlLmxNtL6B4b\nxqBzojmvXTS924SfPIVzjOOodZB2/y/WqH7/L5Cx+eTAb9nTCvpjI/zwNhr4DYyGu1IertRRzpo9\nh/lxVzbf78xmfVoe5U5DgK8XfeMjGeQa2Xdu0QQvr98IaEfpicDfv9YK/YqBHxjhCvsKI/zw1hr4\nNtJwV6qROVJSxoqUXH7Ymc0PO7PZkVkAQFigL33jI+mXEEm/tpF0btEEH+/TLHfgKIWMTRVG+Gut\nPwBOh7U/MNIK++bdoUV3aN7DOhdfl1CoFxruSjVymUdK+GFXNitSclmxO5fd2dZZOMF+3vQ5FvYJ\nkXSPDcfPp4pgLiuBzE0nj/Azt54Y4fuFQLOurrB3hX5MJ/Dxq+NeNj4a7kqpk2QeKWHF7lxW7s5l\nxe4ctmdYI/sAXy96xUWQlBBJUkIkPePCCfb3qfoNHUcha4t1oPbgeutrxkY4ar0vXr7QtKM1sj8W\n+s27gn9oHfbS82m4K6VOK7fwKCtdYb8yNYfN+4/gNODtJXRp2YTENpH0jY8gMT7yxHo4VXE6rWUU\nDq47OfSLsl0NxJrCqTjCb95Dl1Q4AxruSqkzcqSkjF/2HiY5NZdVqbn8svcwpQ4nAPFRQSTGR5IU\nH0lifAQJ0cHWmjjVYQzkH6gQ9uusr4f3nmgT2qJC2HeH5t301MzfoOGulDorRx1ONu3PIzn1EKtS\nc0nec4jcwqMARAX7kRgfQd/4SBLjI+nSsgm+pztIW5niQ3Bww8kj/Oxt1oVXYN2MvHk3K/Bb9oZW\nva1RfyMPfA13pVStMsaQkl1IcmouK3cfInlPLntyioAT8/bHpnF6tQ4ntLILq6pSVmydillxWidj\nEzhKrP2BEdCqj7W8Qqve0KwLNGnVqAJfw10pVecyj5SQvMc1sk89xKb9eTgNeAl0atHENbK3RvjN\nmtRwMbNyh3XgNi0Z0ldbj8wtgCu7AsKsu1k16wLNOltn7UR3gMDwWutnQ6LhrpSqdwWlDtbuPeya\nxsllzZ7DFJeVAxAXGUjfNtY0Tt/4CNrFhPz2xVVVKc23pnQyNlmPzM3WiP9o/ok2AeHWvP2xR2TC\nie+bxIJ3Nc4IaoBqNdxFZDjwDOANvGKMmfmr/ecDs4HuwDhjzKKq3lPDXSnPV1buZMuBI6xKPXT8\nQG12gTVvHx7kS2KbCHrEhtO1VRhdW4VV/6ycyhhjHaTN2AQ5O+FQ6onH4b0nzskHEG8Ijzs5/Cs+\nAiNqXkcdq7VwFxFvYDtwMZAGrALGG2M2V2gTDzQB7gE+0nBXSlXGGMOenKLj0zir9uQeX+IYoHmT\nAFfQN6FbqzC6tQqjaU2ncypylsOR/ScHfsXH8VM1XQLCrJAPbw0hzSC4qXW6ZsXvg5vaspRydcO9\nOv8uSQJ2GmNSXG/8DjAGOB7uxphU1z5njapVSjUKIkJ8dDDx0cH8PjEOgPySMjbvP8KG9Dw2puex\nIT2Pr7ZmcGzcGRPqT+cWTejYPJRzXY9zmoZUvjjab/FyjdTD46ybkf9aaT4c2nNq6Gdtt1bULD5U\n+fv6hZ4I+srCPzgGgiKtfwkEhNfrVFB1PqkVsK/C8zSgX92Uo5RqbEIDfOnXNop+baOObysodbB5\n/xE2ugJ/y8F8ftqVw9Fya/zo7SUkRAdzbvNQOjazAr9j8ybERgTWbB7fP9S6erZ518r3O45ao/uC\nTOtRmHnq91X9IQDr9M7Q5jDkAeh65ZnXeQbq9YiCiEwFpgK0bt26Pj9aKeVGQvx9ji+HcExZuZPU\n7EK2Hsxn28F8th7MZ33aYT5df+B4m2A/b9o3s0b27WJCXF+DaR0ZdPrF0qri4wdNWlqPqjiOQmGW\nFfqFOVbYH3sUZVsXdAWE1byW6pZcjTbpQFyF57GubWfMGDMHmAPWnHtN3kMp1Tj5envRvlko7ZuF\nclmPE9sLSh1sz7AC/9jj2+1ZLFqdVuG1QnxU8InAb2p93y4mpHrr6JwJHz8Ia2U9bFSdXq0C2otI\nAlaojwMm1GlVSilVTSH+PvRuHUHv1ief4ZJXXEZKVgG7sgrZmVnArqwCtmfks3RLBuXOE2PLFmEB\nJ43y28WEEB8dTPMmATU/VbMBqDLcjTEOEbkdWIx1KuRcY8wmEXkESDbGfCQifYEPgAjgMhF52BjT\npU4rV0qp0wgL9KVX6wh6/Sr0jzqc7M09Fvgngv/d5H0UHi0/3i7A14v4qGDrER1MQnQQCdEhxEcH\nERPiX/21dWyiFzEppRTWaZoHj5SQklXI7mzrkZpdyO6cQvblFlFWfiIrQ/x9aBMVREJ0MAnR1h+A\nhBjra0SQb50Gf22eCqmUUh5PRGgRFkiLsEAGnhN90j5HuZP9h0tIyS4gNbuQ1JwidmcXsiE9j882\nHKDCLA8h/j60jgyidWQQbaKCiHN9bRMZTIvwgDNfYK2GNNyVUqoKPt5etI4KonVUEJx78r6jDif7\nDhWRml3Inpwi9uYWsSenkB2Z+Xy9LZOjjhOX/3h7Ca3CA/nLJR0Y07NuD7hquCul1Fnw8/E6fubN\nrzmd1lTP3twi9h4L/twiokPOYpmFatJwV0qpOuLlJbQMD6RleCD9K1ykVS+fXa+fppRSql5ouCul\nlAfScFdKKQ+k4a6UUh5Iw10ppTyQhrtSSnkgDXellPJAGu5KKeWBbFs4TESygD01fHk0kF1lK8+i\nfW4ctM+Nw9n0uY0xJqaqRraF+9kQkeTqrIrmSbTPjYP2uXGojz7rtIxSSnkgDXellPJA7hruc+wu\nwAba58ZB+9w41Hmf3XLOXSml1Om568hdKaXUabhduIvIcBHZJiI7ReQBu+upLSIyV0QyRWRjhW2R\nIrJURHa4vka4touIPOv6GawXkd72VV5zIhInIstEZLOIbBKRu1zbPbbfIhIgIitFZJ2rzw+7tieI\nyApX3xaIiJ9ru7/r+U7X/ng7668pEfEWkV9E5BPXc4/uL4CIpIrIBhFZKyLJrm319rvtVuEuIt7A\nC8AIoDMwXkQ621tVrXkdGP6rbQ8AXxlj2gNfuZ6D1f/2rsdU4D/1VGNtcwB/McZ0BvoDt7n+e3py\nv0uBYcaYHkBPYLiI9AeeAJ42xpwDHAJudLW/ETjk2v60q507ugvYUuG5p/f3mKHGmJ4VTnusv99t\nY4zbPIABwOIKz6cB0+yuqxb7Fw9srPB8G9DC9X0LYJvr+/8C4ytr584P4H/AxY2l30AQsAboh3VB\ni49r+/Hfc2AxMMD1vY+rndhd+xn2M9YVZMOATwDx5P5W6HcqEP2rbfX2u+1WI3egFbCvwvM01zZP\n1cwYc8D1/UGgmet7j/s5uP753QtYgYf32zVFsRbIBJYCu4DDxhiHq0nFfh3vs2t/HlC/92s7e7OB\n+4Bjd4qOwrP7e4wBlojIahGZ6tpWb7/beg9VN2GMMSLikac2iUgI8B7wJ2PMERE5vs8T+22MKQd6\nikg48AHQ0eaS6oyI/A7INMasFpEhdtdTzwYZY9JFpCmwVES2VtxZ17/b7jZyTwfiKjyPdW3zVBki\n0gLA9TXTtd1jfg4i4osV7G8bY953bfb4fgMYYw4Dy7CmJcJF5Nhgq2K/jvfZtT8MyKnnUs/GQGC0\niKQC72BNzTyD5/b3OGNMuutrJtYf8STq8Xfb3cJ9FdDedaTdDxgHfGRzTXXpI2CS6/tJWHPSx7Zf\n5zrC3h/Iq/BPPbch1hD9VWCLMWZWhV0e228RiXGN2BGRQKxjDFuwQv4qV7Nf9/nYz+Iq4GvjmpR1\nB8aYacaYWGNMPNb/r18bY67BQ/t7jIgEi0jose+BS4CN1Ofvtt0HHWpwkGIksB1rnvJBu+upxX7N\nBw4AZVjzbTdizTV+BewAvgQiXW0F66yhXcAGINHu+mvY50FY85LrgbWux0hP7jfQHfjF1eeNwHTX\n9rbASmAn8C7g79oe4Hq+07W/rd19OIu+DwE+aQz9dfVvneux6VhW1efvtl6hqpRSHsjdpmWUUkpV\ng4a7Ukp5IA13pZTyQBruSinlgTTclVLKA2m4K6WUB9JwV0opD6ThrpRSHuj/AeyT2/pXZVRMAAAA\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3C9b_lNt5hv",
        "colab_type": "code",
        "outputId": "c20aef23-e242-43ed-e402-74a7deb53197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        }
      },
      "source": [
        "# GPU vs CPU comparison\n",
        "\n",
        "import time\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "#CPU\n",
        "myNN = NeuralNetwork(size_inputs, size_outputs)\n",
        "optimizer = torch.optim.Adam(myNN.parameters(), lr=0.001)\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(num_epochs):\n",
        "  _, train_loss = train_test(myNN, train_x, train_y, optimizer, criterion, mode='train')\n",
        "  train_losses.append(train_loss)\n",
        "  \n",
        "  _, test_loss = train_test(myNN, test_x, test_y, criterion=criterion, mode='test')\n",
        "  test_losses.append(test_loss)\n",
        "\n",
        "end_time = time.time() - start_time\n",
        "print(\"CPU training 100 epochs takes\", end_time,\"seconds.\")\n",
        "  \n",
        "plt.plot(train_losses, label='training loss')\n",
        "plt.plot(test_losses, label='test loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#GPU\n",
        "myNN = NeuralNetwork(size_inputs, size_outputs)\n",
        "optimizer = torch.optim.Adam(myNN.parameters(), lr=0.001)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  myNN = myNN.cuda()\n",
        "  train_x, train_y = train_x.cuda(), train_y.cuda()\n",
        "  test_x, test_y = test_x.cuda(), test_y.cuda()\n",
        "  \n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(num_epochs):\n",
        "  _, train_loss = train_test(myNN, train_x, train_y, optimizer, criterion, mode='train')\n",
        "  train_losses.append(train_loss)\n",
        "  \n",
        "  _, test_loss = train_test(myNN, test_x, test_y, criterion=criterion, mode='test')\n",
        "  test_losses.append(test_loss)\n",
        "\n",
        "end_time = time.time() - start_time\n",
        "print(\"GPU training 100 epochs takes\", end_time,\"seconds.\")\n",
        "  \n",
        "plt.plot(train_losses, label='training loss')\n",
        "plt.plot(test_losses, label='test loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU training 100 epochs takes 2.2213473320007324 seconds.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpr0lVLKiWjSV0opJ/J/WiVCQJUHQ8gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "GPU training 100 epochs takes 0.46485280990600586 seconds.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVfUOrCQvEHf",
        "colab_type": "code",
        "outputId": "a5592c68-3829-48a9-fd99-71d93f4423ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Support Vector Machine attempt to compare to Neural Network\n",
        "# Note: Best practices typical for Machine Learning were not \n",
        "# applied to this demo. While SVM may look weaker, I expect \n",
        "# that stronger data analysis will improve it greatly.\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(myData, y_map, test_size=0.2)\n",
        "#[6, 14, 15, ... 92, 95] -> [0, 1, 2, ..., 12, 13]\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "yTest = to_categorical(yTest, 14)\n",
        "\n",
        "clf = svm.SVC(max_iter=20000)\n",
        "clf.fit(xTrain, yTrain)\n",
        "preds = clf.predict(xTest)\n",
        "preds = to_categorical(preds, 14)\n",
        "\n",
        "print(criterion(torch.FloatTensor(preds), torch.FloatTensor(yTest)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.3646)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}